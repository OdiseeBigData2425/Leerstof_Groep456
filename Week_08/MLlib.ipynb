{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a82620d",
   "metadata": {},
   "source": [
    "# MLlib\n",
    "\n",
    "Spark biedt ook een framework aan voor MachineLearning modellen te trainen op gedistribueerde datasets.\n",
    "Dit framework is MLlib of ook wel sparkML genoemd.\n",
    "De code om te werken met deze package is sterk gelijkaardig aan sklearn.\n",
    "De API en een uitgebreide documentatie met voorbeeldcode kan je [hier](https://spark.apache.org/docs/latest/ml-guide.html) vinden.\n",
    "\n",
    "Deze package bied de volgende tools aan\n",
    "* ML-technieken: classificatie, regressie, clustering, ...\n",
    "* Features: Extracting en transforming van features, PCA, ...\n",
    "* Pipelines: Maak, train, optimaliseer en evalueer pipelines\n",
    "* Persistentie: Bewaar en laden van algoritmes/modellen\n",
    "* Databeheer: Algebra tools, statistieken, null-waarden, ...\n",
    "\n",
    "Let op dat er twee API's aangeboden worden, 1 gebaseerd op RDD's en 1 op DataFrames.\n",
    "De API gebaseerd op RDD's is ouder en minder flexibel dan de API gebruik makend van DataFrames.\n",
    "Momenteel werken ze allebei maar in de toekomst zou de RDD gebaseerde kunnen verdwijnen.\n",
    "\n",
    "## Utilities\n",
    "\n",
    "### Varianten voor numpy-arrays\n",
    "\n",
    "Voor feature sets en volledige matrices van datasets aan te maken kan je gebruik maken van de Vector en Matrix klassen.\n",
    "Deze beschikken over een Dense variant waar je elk element moet ingeven of een Sparse Variant waar cellen, elementen leeg kan laten.\n",
    "Dit ziet er als volgt uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef759da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/06 11:56:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName(\"MLLib intro\").config(\"spark.driver.memory\", \"4g\").config(\"spark.executor.memory\", \"4g\").config(\"spark.executor.memoryOverhead\", \"1g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0143ac7",
   "metadata": {},
   "source": [
    "Het is belangrijk om te weten dat dit locale datastructuren (wrapper rond numpy array) zijn en geen gedistribueerde objecten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e9fe3a-921e-4bea-8e6e-7103b1ac4d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,[0,1],[2.0,1.0])\n",
      "[2.0,1.0,0.0,0.0]\n",
      "DenseMatrix([[ 0.,  4.,  8., 12.],\n",
      "             [ 1.,  5.,  9., 13.],\n",
      "             [ 2.,  6., 10., 14.],\n",
      "             [ 3.,  7., 11., 15.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, Matrices\n",
    "\n",
    "data = Vectors.sparse(4, (0, 1), (2.0, 1.0))\n",
    "print(data)\n",
    "data = Vectors.dense([2.0, 1.0, 0.0, 0.0])\n",
    "print(data)\n",
    "data = Matrices.dense(4, 4, range(16))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5676b",
   "metadata": {},
   "source": [
    "### Statistieken\n",
    "\n",
    "Voor er kan gewerkt worden met statistieken moeten we (net zoals bij pandas) eerst een dataset hebben.\n",
    "Hieronder maken we een random dataframe aan van 50 rijen en 4 kolommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91673fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+-------------------+\n",
      "|summary|                  _1|                  _2|                  _3|                 _4|\n",
      "+-------+--------------------+--------------------+--------------------+-------------------+\n",
      "|  count|                  50|                  50|                  50|                 50|\n",
      "|   mean|  0.5277823634936698| 0.45039698313032434| 0.43083175420665226| 0.5033761310058955|\n",
      "| stddev| 0.28803317364991815| 0.30465666667986957| 0.23551772800012008|0.27961638439500036|\n",
      "|    min|0.027289600150978033|0.006537536790830023|0.010302092502849414|0.02558399730345995|\n",
      "|    max|  0.9742260064078323|  0.9830205082146971|  0.9842047921655179| 0.9914768379236777|\n",
      "+-------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# random data aanmaken (50 rijen en 4 kolommen)\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "sc = spark.sparkContext\n",
    "data = RandomRDDs.uniformVectorRDD(sc, 50, 4).map(lambda a: a.tolist()).toDF()\n",
    "data.head(5)\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3fad4",
   "metadata": {},
   "source": [
    "**Correlation matrix**\n",
    "\n",
    "Buiten de statistieken die berekend kunnen worden door de summary() functie kan ook de correlatiematrix belangrijk zijn.\n",
    "Deze matrix maakt het mogelijk om het verband tussen de verscheidene features te bestuderen.\n",
    "Deze matrix kan als volgt berekend worden voor een gedistribueerd dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b6287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+--------------------------------------------------------------------------------+\n",
      "|_1                  |_2                  |_3                 |_4                 |vector                                                                          |\n",
      "+--------------------+--------------------+-------------------+-------------------+--------------------------------------------------------------------------------+\n",
      "|0.4220033153845991  |0.35004689001896083 |0.36866243345675354|0.6729241647864656 |[0.4220033153845991,0.35004689001896083,0.36866243345675354,0.6729241647864656] |\n",
      "|0.3844623109357628  |0.6350806128449099  |0.6913926352466443 |0.7664217645523693 |[0.3844623109357628,0.6350806128449099,0.6913926352466443,0.7664217645523693]   |\n",
      "|0.1258495827205075  |0.2726724908455812  |0.7217576619785252 |0.7844931480053702 |[0.1258495827205075,0.2726724908455812,0.7217576619785252,0.7844931480053702]   |\n",
      "|0.44111056411460436 |0.06495557377755123 |0.3783184879152791 |0.4288061046585595 |[0.44111056411460436,0.06495557377755123,0.3783184879152791,0.4288061046585595] |\n",
      "|0.04201175951500413 |0.8382688787875853  |0.9842047921655179 |0.4274475062527101 |[0.04201175951500413,0.8382688787875853,0.9842047921655179,0.4274475062527101]  |\n",
      "|0.8766253216566182  |0.36272491389074757 |0.2084328033390458 |0.384389490728792  |[0.8766253216566182,0.36272491389074757,0.2084328033390458,0.384389490728792]   |\n",
      "|0.850873983744817   |0.3252512491892874  |0.27642213434971596|0.41865504735177017|[0.850873983744817,0.3252512491892874,0.27642213434971596,0.41865504735177017]  |\n",
      "|0.8220623826669125  |0.6298863182126198  |0.35792570945907065|0.7765333489474877 |[0.8220623826669125,0.6298863182126198,0.35792570945907065,0.7765333489474877]  |\n",
      "|0.8938846326910669  |0.18012115123943107 |0.5127315577297528 |0.0800978287717995 |[0.8938846326910669,0.18012115123943107,0.5127315577297528,0.0800978287717995]  |\n",
      "|0.3488703073430659  |0.22567798016989538 |0.37783381294665086|0.03263315598122507|[0.3488703073430659,0.22567798016989538,0.37783381294665086,0.03263315598122507]|\n",
      "|0.027289600150978033|0.9830205082146971  |0.3312469373793261 |0.29048836317385995|[0.027289600150978033,0.9830205082146971,0.3312469373793261,0.29048836317385995]|\n",
      "|0.240363197202334   |0.5692070038339     |0.2953511556323727 |0.8245692117642399 |[0.240363197202334,0.5692070038339,0.2953511556323727,0.8245692117642399]       |\n",
      "|0.8505601410411946  |0.30563342747000555 |0.7299224093040351 |0.9914768379236777 |[0.8505601410411946,0.30563342747000555,0.7299224093040351,0.9914768379236777]  |\n",
      "|0.06441368305620876 |0.7958308242285806  |0.4871840249223436 |0.5682407147453695 |[0.06441368305620876,0.7958308242285806,0.4871840249223436,0.5682407147453695]  |\n",
      "|0.720762544985707   |0.6003143813042965  |0.47774447028124023|0.12538795936072133|[0.720762544985707,0.6003143813042965,0.47774447028124023,0.12538795936072133]  |\n",
      "|0.28207640228484443 |0.7328542981066093  |0.3811025187923627 |0.41072290609208006|[0.28207640228484443,0.7328542981066093,0.3811025187923627,0.41072290609208006] |\n",
      "|0.1793237084820959  |0.040357752745672104|0.561791755695611  |0.732726861223601  |[0.1793237084820959,0.040357752745672104,0.561791755695611,0.732726861223601]   |\n",
      "|0.10776669276658446 |0.9465183485021633  |0.49839634602786875|0.7171707023092013 |[0.10776669276658446,0.9465183485021633,0.49839634602786875,0.7171707023092013] |\n",
      "|0.16588642801297993 |0.006537536790830023|0.35873300759933   |0.3773714328846861 |[0.16588642801297993,0.006537536790830023,0.35873300759933,0.3773714328846861]  |\n",
      "|0.8474943633063637  |0.03153979629206438 |0.3497780427615228 |0.5685300159313932 |[0.8474943633063637,0.03153979629206438,0.3497780427615228,0.5685300159313932]  |\n",
      "+--------------------+--------------------+-------------------+-------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.        , -0.15261255, -0.19830636, -0.12312731, -0.15261255,\n",
       "        1.        ,  0.02824971,  0.17790507, -0.19830636,  0.02824971,\n",
       "        1.        ,  0.15273451, -0.12312731,  0.17790507,  0.15273451,\n",
       "        1.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=data.columns, outputCol='vector')\n",
    "df_vector = assembler.transform(data)\n",
    "df_vector.show(truncate=False)\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "df_corr = Correlation.corr(df_vector, 'vector')\n",
    "df_corr.collect()[0][0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0084e1",
   "metadata": {},
   "source": [
    "**Onafhankelijksheidtest**\n",
    "\n",
    "Naast de correlatiematrix kan het ook belangrijk zijn om de onafhankelijkheid te testen tussen elke feature en een label.\n",
    "Dit kan uitgevoerd worden door een zogenaamde ChiSquareTest.\n",
    "Deze krijgt als input een dataframe, de naam van de kolom met de features (als vectors) en de naam van een kolom met de labels.\n",
    "We kunnen deze test uitvoeren als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8412ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+--------------------+-----+\n",
      "|                  _1|                  _2|                 _3|                 _4|              vector|label|\n",
      "+--------------------+--------------------+-------------------+-------------------+--------------------+-----+\n",
      "|  0.4220033153845991| 0.35004689001896083|0.36866243345675354| 0.6729241647864656|[0.42200331538459...|    0|\n",
      "|  0.3844623109357628|  0.6350806128449099| 0.6913926352466443| 0.7664217645523693|[0.38446231093576...|    1|\n",
      "|  0.1258495827205075|  0.2726724908455812| 0.7217576619785252| 0.7844931480053702|[0.12584958272050...|    1|\n",
      "| 0.44111056411460436| 0.06495557377755123| 0.3783184879152791| 0.4288061046585595|[0.44111056411460...|    1|\n",
      "| 0.04201175951500413|  0.8382688787875853| 0.9842047921655179| 0.4274475062527101|[0.04201175951500...|    1|\n",
      "|  0.8766253216566182| 0.36272491389074757| 0.2084328033390458|  0.384389490728792|[0.87662532165661...|    0|\n",
      "|   0.850873983744817|  0.3252512491892874|0.27642213434971596|0.41865504735177017|[0.85087398374481...|    1|\n",
      "|  0.8220623826669125|  0.6298863182126198|0.35792570945907065| 0.7765333489474877|[0.82206238266691...|    1|\n",
      "|  0.8938846326910669| 0.18012115123943107| 0.5127315577297528| 0.0800978287717995|[0.89388463269106...|    1|\n",
      "|  0.3488703073430659| 0.22567798016989538|0.37783381294665086|0.03263315598122507|[0.34887030734306...|    1|\n",
      "|0.027289600150978033|  0.9830205082146971| 0.3312469373793261|0.29048836317385995|[0.02728960015097...|    1|\n",
      "|   0.240363197202334|     0.5692070038339| 0.2953511556323727| 0.8245692117642399|[0.24036319720233...|    0|\n",
      "|  0.8505601410411946| 0.30563342747000555| 0.7299224093040351| 0.9914768379236777|[0.85056014104119...|    1|\n",
      "| 0.06441368305620876|  0.7958308242285806| 0.4871840249223436| 0.5682407147453695|[0.06441368305620...|    1|\n",
      "|   0.720762544985707|  0.6003143813042965|0.47774447028124023|0.12538795936072133|[0.72076254498570...|    0|\n",
      "| 0.28207640228484443|  0.7328542981066093| 0.3811025187923627|0.41072290609208006|[0.28207640228484...|    1|\n",
      "|  0.1793237084820959|0.040357752745672104|  0.561791755695611|  0.732726861223601|[0.17932370848209...|    0|\n",
      "| 0.10776669276658446|  0.9465183485021633|0.49839634602786875| 0.7171707023092013|[0.10776669276658...|    0|\n",
      "| 0.16588642801297993|0.006537536790830023|   0.35873300759933| 0.3773714328846861|[0.16588642801297...|    0|\n",
      "|  0.8474943633063637| 0.03153979629206438| 0.3497780427615228| 0.5685300159313932|[0.84749436330636...|    0|\n",
      "+--------------------+--------------------+-------------------+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, when\n",
    "\n",
    "df = df_vector.withColumn('label', when(rand() > 0.5, 1).otherwise(0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "986bd348-1e36-474f-bf8a-38214e038e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------------+-----------------+\n",
      "|featureIndex|             pValue|degreesOfFreedom|        statistic|\n",
      "+------------+-------------------+----------------+-----------------+\n",
      "|           0|0.43343669725576095|              49|49.99999999999992|\n",
      "|           1|0.43343669725576095|              49|49.99999999999992|\n",
      "|           2|0.43343669725576095|              49|49.99999999999992|\n",
      "|           3|0.43343669725576095|              49|49.99999999999992|\n",
      "+------------+-------------------+----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "ChiSquareTest.test(df, 'vector', 'label', flatten=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd5dd5-04ea-435c-a33b-e1fdeea81e84",
   "metadata": {},
   "source": [
    "De Chi-square test wordt op de volgende manieren gebruikt in Machine Learning:\n",
    "* Feature selectie: Identificeer welke features een significante relatie hebben met de targetvariabele.\n",
    "* Correlatie tussen categorische variabelen: Evalueer de afhankelijkheid tussen twee categorische variabelen.\n",
    "\n",
    "De Chi-square test vergelijkt de verwachte en werkelijke frequenties van waarden in de categorieÃ«n van een feature ten opzichte van de targetvariabele. Het doel is om te bepalen of de verschillen tussen deze frequenties statistisch significant zijn of niet. Een hoge Chi-square score betekent dat er een grote afhankelijkheid is tussen de feature en de target, wat suggereert dat de feature nuttig is voor voorspellingen.\n",
    "Als de p-waarde kleiner is dan een vooraf bepaald significantieniveau (bijvoorbeeld 0,05), verwerp je de nullhypothese (dat er geen relatie is), wat betekent dat de feature relevant is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3640d89",
   "metadata": {},
   "source": [
    "**Summarizer**\n",
    "\n",
    "Andere statistieken per kolom kunnen berekend worden door gebruik te maken van de Summarizer klasse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c81a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+\n",
      "|aggregate_metrics(vector, 1.0)                                                     |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "|{[0.5277823634936697,0.4503969831303244,0.4308317542066523,0.5033761310058955], 50}|\n",
      "+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
    "df.select(summarizer.summary(df.vector)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac099f10",
   "metadata": {},
   "source": [
    "Het gebruik maken van de Summarizer maakt het dus mogelijk om rechtstreeks op de feature vectors te werken zonder ze eerst terug te moeten splitsen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e0343",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Pipelines binnen Spark zijn een groep van high-level API's steunend op Dataframes om ML-pipelines aan te maken, optimaliseren en trainen.\n",
    "De belangrijkste concepten binnen de Pipelines van Spark zijn:\n",
    "* Dataframe: concept van de dataset\n",
    "* Transformer: Zet een dataframe om in een ander dataframe\n",
    "* Estimator: Zet een dataframe om in een model/transformer\n",
    "* Pipeline: een ketting van transformers en estimators om een flow vast te leggen\n",
    "* Parameter: API voor parameters van transformers en estimators aan te passen\n",
    "\n",
    "Gebruik nu onderstaande mini-dataset waar we op basis van een tekstkolom met logistische regressie een bepaald label proberen te voorspellen.\n",
    "Maak hiervoor een Pipeline uit die bestaat uit de volgende stappen:\n",
    "* Tokenizer om de tekstkolom te splitsen in de overeenkomstige woorden\n",
    "* HashingTf om de term frequency van de woorden te bepalen en het om te zetten naar een feature vector\n",
    "* LogisticRegression Estimator om de voorspelling te doen.\n",
    "\n",
    "Train daarna deze pipeline en maak de voorspellingen voor de traningsdata.\n",
    "Hoe accuraat is dit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60c7d2-eff6-494c-bd70-35bd62cdf0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (0, 'wat is big data tof', 1.0),\n",
    "    (1, 'ik kan goed big data', 1.0),\n",
    "    (2, 'ik wil van het zonnetje genieten', 0.0),\n",
    "    (3, 'ik heb dorst',  0.0)\n",
    "], ['id', 'text', 'label'])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='tokens')\n",
    "hasher = HashingTF(inputCol='tokens', outputCol='features')\n",
    "logistic = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "df_prepped = hasher.transform(tokenizer.transform(training))\n",
    "model = logistic.fit(df_prepped) # model is een transformer, logistic is een estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd6641fe-d1df-41e6-af0c-78aabecf5a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|                text|label|              tokens|            features|       rawPrediction|         probability|prediction|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  0| wat is big data tof|  1.0|[wat, is, big, da...|(262144,[67406,67...|[-6.8203767382313...|[0.00109012004814...|       1.0|\n",
      "|  1|ik kan goed big data|  1.0|[ik, kan, goed, b...|(262144,[97142,15...|[-6.2496016696093...|[0.00192750081356...|       1.0|\n",
      "|  2|ik wil van het zo...|  0.0|[ik, wil, van, he...|(262144,[16725,15...|[6.94109921712630...|[0.99903372873981...|       0.0|\n",
      "|  3|        ik heb dorst|  0.0|    [ik, heb, dorst]|(262144,[88144,12...|[6.17763043040952...|[0.99792895842010...|       0.0|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(df_prepped).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcf1e33d-31e1-402e-877b-585d15060b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|                text|label|              tokens|            features|       rawPrediction|         probability|prediction|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  0| wat is big data tof|  1.0|[wat, is, big, da...|(262144,[67406,67...|[-6.8203767382313...|[0.00109012004814...|       1.0|\n",
      "|  1|ik kan goed big data|  1.0|[ik, kan, goed, b...|(262144,[97142,15...|[-6.2496016696093...|[0.00192750081356...|       1.0|\n",
      "|  2|ik wil van het zo...|  0.0|[ik, wil, van, he...|(262144,[16725,15...|[6.94109921712630...|[0.99903372873981...|       0.0|\n",
      "|  3|        ik heb dorst|  0.0|    [ik, heb, dorst]|(262144,[88144,12...|[6.17763043040952...|[0.99792895842010...|       0.0|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hasher, logistic])\n",
    "model = pipeline.fit(training)\n",
    "predictions = model.transform(training)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e7af8",
   "metadata": {},
   "source": [
    "### Evalueren van een model\n",
    "\n",
    "In de pyspark.ml package zitten er ook functionaliteiten voor deze modellen te evalueren.\n",
    "Meer informatie hierover vind je [hier](https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd4bd140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a305ea2",
   "metadata": {},
   "source": [
    "### Data sources\n",
    "\n",
    "Door gebruik te maken van de sparkContext kunnen een reeks standaard databronnen ingelezen worden om datasets uit op te bouwen (Csv, Json, ...).\n",
    "Daarnaast is het ook mogelijk om een folder met een reeks beelden te gebruiken als dataset om zo een model voor image classification te trainen.\n",
    "Download nu [deze](https://www.kaggle.com/returnofsputnik/chihuahua-or-muffin) dataset en upload ze naar een folder op het hadoop filesysteem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/returnofsputnik/chihuahua-or-muffin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62e4f42a-ffec-4f23-8b48-ccd3c2a043d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/bigdata/MLLib/chihuahua-or-muffin'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "client = InsecureClient('http://localhost:9870', user='bigdata')\n",
    "map = \"/user/bigdata/MLLib\"\n",
    "\n",
    "if client.status(map, strict=False) is None:\n",
    "    client.makedirs(map)\n",
    "else:\n",
    "    # do some cleaning in case anything else than *.txt is present\n",
    "    for f in client.list(map):\n",
    "        client.delete(map + '/' + f, recursive=True)\n",
    "\n",
    "client.upload(map, 'chihuahua-or-muffin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70542e14",
   "metadata": {},
   "source": [
    "De geuploade images kunnen nu ingelezen worden als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad93c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n",
      "+-----+\n",
      "|width|\n",
      "+-----+\n",
      "|  172|\n",
      "|  171|\n",
      "|  171|\n",
      "|  172|\n",
      "|  172|\n",
      "|  168|\n",
      "|  168|\n",
      "|  171|\n",
      "|  168|\n",
      "|  171|\n",
      "|  171|\n",
      "|  171|\n",
      "|  168|\n",
      "|  171|\n",
      "|  171|\n",
      "|  172|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('image').load('/user/bigdata/MLLib/chihuahua-or-muffin')\n",
    "df.printSchema()\n",
    "df.select('image.width').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a67bf4",
   "metadata": {},
   "source": [
    "Merk op dat het werken met images niet zo eenvoudig is.\n",
    "Hiervoor wordt binnen pyspark typisch gebruik gemaakt van de [sparkdl](https://smurching.github.io/spark-deep-learning/site/api/python/sparkdl.html) package.\n",
    "Hierbij staat de dl voor deep learning.\n",
    "Aangezien dit ons momenteel te ver leidt ga ik dit niet verder toelichten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4b653",
   "metadata": {},
   "source": [
    "Een andere aparte databron die eenvoudig ingelezen kan worden is het formaat \"libsvm\".\n",
    "Een bestand van dit formaat wordt ingelezen als een dataframe met twee kolommen: een label en een kolom met de feature-vectors.\n",
    "De code om dergelijk bestand in te laden is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('libsvm').load('{path to file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baaeb6d-3fcf-4300-b25e-500cb5109c0d",
   "metadata": {},
   "source": [
    "## Voorbeeld\n",
    "\n",
    "Train en evalueer een machine learning model dat beeldgegevens kan classificeren, gebaseerd op eigenschappen zoals hoogte, breedte en aantal kanalen. Omdat MLlib geen directe ondersteuning biedt voor beelddata, zullen we enkele numerieke eigenschappen van de afbeeldingen gebruiken en pipelines opzetten voor preprocessing en training.\n",
    "Hierbij ga je verder werken op het dataframe van de chichuahua of muffin dataset van hierboven.\n",
    "\n",
    "Voer de volgende stappen uit:\n",
    "* Data decoderen en omzetten naar een dataframe dat door ML-kan gebruikt worden.\n",
    "    * Schrijf een udf om de image.data kolom om te zetten naar een vector. Gebruik hiervoor een udf dat dit eerst omzet naar een numpy array waarna het omgezet wordt naar een PIL-Image (een python package). Dit wordt dan geresized naar een 32x32 figuur. Ten slotte wordt dit omgezet naar een 1D-array \n",
    "    * Schrijf een udf om de filename in de image.origin kolom om te zetten naar 1 van de labels: 'muffin' of 'chihuahua'\n",
    "* Na het uitvoeren van de udf's zou je een dataframe moeten hebben met minstens 2 kolommen: het label als string kolom, de data als een vector kolom\n",
    "* Splits de data in een trainings en testset\n",
    "* Voer de volgende preprocessing stappen uit\n",
    "    * Zet de tekstuele labels om naar integers\n",
    "    * Schaal de vector-kolom door elke waarde te delen door 255 zodat de pixel-waarden in de range 0-255 vallen\n",
    "    * Zorg ervoor dat dit in een pipeline zit en fit de preprocessor al eens. Print het schema en een aantal rijen uit om de output te verifieren\n",
    "* Voer data modelling uit\n",
    "    * Zorg voor dimensionality reduction aan de hand van PCA\n",
    "    * Gebruik logistische regressie om het label te voorspellen\n",
    "* Evalueer het model door een confusion matrix te berekenen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f7891fe-0831-4a84-ab0e-65ed1fca2ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 13:31:48 ERROR Executor: Exception in task 0.0 in stage 116.0 (TID 750)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`functions$$$Lambda$2967/352528797`: (array<double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.ml.functions$.$anonfun$arrayToVectorUdf$1(functions.scala:76)\n",
      "\t... 20 more\n",
      "25/05/06 13:31:48 WARN TaskSetManager: Lost task 0.0 in stage 116.0 (TID 750) (f46b4845c1e0 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`functions$$$Lambda$2967/352528797`: (array<double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.ml.functions$.$anonfun$arrayToVectorUdf$1(functions.scala:76)\n",
      "\t... 20 more\n",
      "\n",
      "25/05/06 13:31:48 ERROR TaskSetManager: Task 0 in stage 116.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1197.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 116.0 failed 1 times, most recent failure: Lost task 0.0 in stage 116.0 (TID 750) (f46b4845c1e0 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`functions$$$Lambda$2967/352528797`: (array<double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.functions$.$anonfun$arrayToVectorUdf$1(functions.scala:76)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.GeneratedMethodAccessor231.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`functions$$$Lambda$2967/352528797`: (array<double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.functions$.$anonfun$arrayToVectorUdf$1(functions.scala:76)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 3. Pas de resize UDF toe om een nieuwe kolom 'resized_vector' te maken met de resized pixelwaarden\u001b[39;00m\n\u001b[1;32m     39\u001b[0m df_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage.origin\u001b[39m\u001b[38;5;124m\"\u001b[39m, array_to_vector(resize_image_udf(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage.data\u001b[39m\u001b[38;5;124m'\u001b[39m),F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage.width\u001b[39m\u001b[38;5;124m'\u001b[39m),F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage.height\u001b[39m\u001b[38;5;124m'\u001b[39m)))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 40\u001b[0m \u001b[43mdf_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m df_data \u001b[38;5;241m=\u001b[39m df_data\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, get_label_udf(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# splitsen in train en test\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1197.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 116.0 failed 1 times, most recent failure: Lost task 0.0 in stage 116.0 (TID 750) (f46b4845c1e0 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`functions$$$Lambda$2967/352528797`: (array<double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.functions$.$anonfun$arrayToVectorUdf$1(functions.scala:76)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.GeneratedMethodAccessor231.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`functions$$$Lambda$2967/352528797`: (array<double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.functions$.$anonfun$arrayToVectorUdf$1(functions.scala:76)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, MinMaxScaler, PCA\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "target_size=(32,32)\n",
    "num_channels=3\n",
    "num_flattened=target_size[0]*target_size[1]*num_channels\n",
    "\n",
    "def get_label(path):\n",
    "    filename = path.split('/')[-1]\n",
    "    if 'muffin' in filename:\n",
    "        return 'muffin'\n",
    "    else:\n",
    "        return 'chihuahau'\n",
    "\n",
    "def resize_image(pixel_data, width, height, target_size=(32, 32)):\n",
    "    # Convert the pixel data to a numpy array and reshape it to (height, width)\n",
    "    image_array = np.array(pixel_data, dtype=np.uint8).reshape((height, width, 3))\n",
    "    # Convert the numpy array to a PIL Image\n",
    "    image = Image.fromarray(image_array, mode=\"RGB\")  # \"L\" for grayscale images\n",
    "    # Resize the image to the target size\n",
    "    image = image.resize(target_size)\n",
    "    # image to list\n",
    "    image = np.array(image, dtype=float).flatten().tolist()\n",
    "\n",
    "# 2. UDF registreren voor resizing\n",
    "resize_image_udf = F.udf(resize_image, ArrayType(DoubleType()))\n",
    "get_label_udf = F.udf(get_label, StringType())\n",
    "\n",
    "# 3. Pas de resize UDF toe om een nieuwe kolom 'resized_vector' te maken met de resized pixelwaarden\n",
    "df_data = df.select(\"image.origin\", array_to_vector(resize_image_udf(F.col('image.data'),F.col('image.width'),F.col('image.height'))).alias(\"data\"))\n",
    "df_data.show()\n",
    "df_data = df_data.withColumn('label', get_label_udf(F.col('origin')))\n",
    "\n",
    "# splitsen in train en test\n",
    "train_data, test_data = df_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# preprocessing\n",
    "label_indexer = StringIndexer(inputCol='label', outputCol='label_index')\n",
    "scaler = MinMaxScaler(inputCol='data', outputCol='scaled_features')\n",
    "preprocessor = Pipeline(stages=[label_indexer, scaler])\n",
    "\n",
    "preprocessor_model = preprocessor.fit(train_data)\n",
    "train_data_pre = preproccesor_model.transform(train_data)\n",
    "test_data_pre = preproccesor_model.transform(test_data)\n",
    "\n",
    "# modelling\n",
    "pca = PCA(k=5, inputCol='scaled_features', outputCol='pca_features')\n",
    "lr = LogisticRegression(featuresCol='pca_features', labedlCol='label', maxIter=5)\n",
    "pipeline = Pipeline(stages = [preprocessor, pca, lr])\n",
    "\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# evaluate the model\n",
    "preds_and_labels = predictions.select(\"prediction\", \"label_index\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "metrics = MultiClassMetrics()\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(metrics.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d657168-fc7a-4651-a6a1-f6c110160eda",
   "metadata": {},
   "source": [
    "## Oefening - classificatie\n",
    "\n",
    "Voer de volgende stappen uit, volg de best-practices van het data science vak:\n",
    "* Genereer een dataset met de make_classification functie van Scikit-Learn (of gebruik randomSplit op een bestaande dataset in PySpark).\n",
    "* Converteer de dataset naar een PySpark DataFrame.\n",
    "* Voer basisverkenning uit: toon het schema van de data, bekijk de eerste rijen, en controleer de kolomtypes.\n",
    "* Gebruik VectorAssembler om alle features in Ã©Ã©n vector-kolom (features) te combineren.\n",
    "* Controleer de structuur van de output.\n",
    "* Train een Logistic Regression model op de gegenereerde dummy data.\n",
    "* Gebruik een training/test split van 80/20.\n",
    "* Evalueer het model door de nauwkeurigheid (accuracy) te berekenen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b2aca-c84b-4077-b84a-4474974f2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Maak dummy data\n",
    "X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n",
    "df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(5)])\n",
    "df['label'] = y\n",
    "\n",
    "# Converteer naar Spark DataFrame\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "# Basisexploratie\n",
    "df_spark.printSchema()\n",
    "df_spark.show(5)\n",
    "\n",
    "# Maak een assembler om features samen te voegen\n",
    "assembler = VectorAssembler(inputCols=[f\"feature_{i}\" for i in range(5)], outputCol=\"features\")\n",
    "df_prepared = assembler.transform(df_spark)\n",
    "\n",
    "# Bekijk het resultaat\n",
    "df_prepared.select(\"features\", \"label\").show(5, truncate=False)\n",
    "\n",
    "# Train/test split\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Voorspellingen en evaluatie\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83900803-722e-4cf2-b123-bbaaa9fab236",
   "metadata": {},
   "source": [
    "## Oefening - hyperparameter tuning\n",
    "\n",
    "Voer de volgende stappen uit, volg de best-practices van het data science vak:\n",
    "* Genereer dummy data (of gebruik bestaande data) om een regressieprobleem op te lossen. (make_regression)\n",
    "* Bouw een pipeline die de data voorbereidt door schaling uit te voeren, een RandomForest-model traint, en de beste hyperparameters selecteert. (Tip: ParamGridBuilder)\n",
    "* Voer hyperparameter tuning uit en evalueer de prestaties van het beste model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977b6e7-449f-4126-9fe8-89da6a101d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start een Spark sessie\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Genereer dummy regressie data\n",
    "X, y = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42)\n",
    "df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(5)])\n",
    "df['label'] = y\n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "# Stap 1: Data voorbereiden met VectorAssembler en MinMaxScaler\n",
    "assembler = VectorAssembler(inputCols=[f\"feature_{i}\" for i in range(5)], outputCol=\"features_vector\")\n",
    "scaler = MinMaxScaler(inputCol=\"features_vector\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Stap 2: RandomForestRegressor model\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
    "\n",
    "# Stap 3: Pipeline opbouwen\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# Stap 4: Hyperparameter grid instellen voor RandomForestRegressor\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Stap 5: CrossValidator instellen\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=3)  # 3-fold cross-validation\n",
    "\n",
    "# Train/test split\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Stap 6: Model trainen en tunen\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Stap 7: Het beste model toepassen op de test set\n",
    "best_model = cv_model.bestModel\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Evaluatie van het beste model met RMSE (Root Mean Squared Error)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Beste model RMSE op test data: {rmse}\")\n",
    "\n",
    "# Beste hyperparameters weergeven\n",
    "print(\"Beste hyperparameters:\")\n",
    "print(\"Aantal bomen (numTrees):\", best_model.stages[-1]._java_obj.getNumTrees())\n",
    "print(\"Maximale diepte (maxDepth):\", best_model.stages[-1]._java_obj.getMaxDepth())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
