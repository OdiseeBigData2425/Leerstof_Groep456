{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark\n",
    "\n",
    "Hoewel het MapReduce algoritme van Hadoop een aantal voordelen heeft. \n",
    "De meest beperkende eigenschap van het MapReduce algoritme is de snelheid.\n",
    "Omdat alles ingelezen wordt vanaf de harde schijf, tussenresultaten op de schijf opgeslagen worden en de finale resultaten ook wordt er tot wel 90% van de rekentijd gespendeerd in lees- of schrijfopdrachten.\n",
    "\n",
    "Spark is geintroduceerd om dit te versnellen door gebruik te maken van in-memory processing.\n",
    "Hierdoor is Spark tot 3 keer sneller op grote datasets en tot 100 keer op kleinere datasets.\n",
    "\n",
    "Het spark framework kan gebruik maken van een externe opslag-locatie voor bestanden bij te houden (zoals HDFS) en bestaat uit de volgende componenten:\n",
    "* SparkCore\n",
    "* Spark SQL\n",
    "* Spark Streaming\n",
    "* MLlib\n",
    "* SparkGraph\n",
    "\n",
    "Daarnaast zijn er ook verschillende Spark Api's voor verschillende programmeertalen zoals Python, Scala, Java, ...\n",
    "Hierdoor is het framework ook flexibeler dan het standaard MapReduce algoritme.\n",
    "Heel veel informatie over het spark framework vind je in de [documentatie](https://spark.apache.org/docs/latest/quick-start.html) en de programming guides (bovenaan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World,\n",
      "hello world,\n",
      "hello world,\n",
      "\n",
      "Dit is een voorbeeld file om het Wordcount voorbeeld te testen !\n"
     ]
    }
   ],
   "source": [
    "map = 'Spark'\n",
    "\n",
    "client = InsecureClient('http://localhost:9870', user='bigdata')\n",
    "\n",
    "if client.status(map, strict=False) is None:\n",
    "    client.makedirs(map)\n",
    "else:\n",
    "    # do some cleaning in case anything else than *.txt is present\n",
    "    for f in client.list(map):\n",
    "        client.delete(map + '/' + f, recursive=True)\n",
    "\n",
    "client.upload(map, 'input.txt')\n",
    "client.upload(map, 'input.csv')\n",
    "client.upload(map, 'titanic.csv')\n",
    "with client.read(map + '/input.txt') as reader:\n",
    "    content = reader.read()\n",
    "    print(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Installatie - reeds gebeurd\n",
    "\n",
    "Een python implementatie van Spark kan eenvoudig geinstalleerd worden door het volgende commando uit te voeren. \n",
    "Dit moet maar eenmalig gebeuren.\n",
    "Om te kijken of het reeds geinstalleerd is kan je kijken naar de versie van pyspark (indien geinstalleerd). \n",
    "Als de versie correct gereturned wordt, dan is het reeds geinstalleerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.3\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 1.8.0_422\n",
      "Branch HEAD\n",
      "Compiled by user haejoon.lee on 2024-09-09T05:20:05Z\n",
      "Revision 32232e9ed33bb16b93ad58cfde8b82e0f07c0970\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version  # controleren of pyspark correct geinstalleerd is (bij mij versie 3.5.3, normaal geen probleem als dit lichtjes anders is (3.x.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark kan op drie manieren werken:\n",
    "* Boven op MapReduce (traag)\n",
    "* Boven op Yarn\n",
    "* Via zijn eigen resource manager\n",
    "\n",
    "In deze notebook gaan we gebruik maken van Spark gebruikmakende van yarn.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext: geeft aan hoe de cluster/storage bereikt kan worden\n",
    "# conf: configuration van de applicatie\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor de configuratie moeten we vooral twee zaken aangeven, namelijk:\n",
    "* Naam van de applicatie (is zichtbaar in de yarn)\n",
    "* Master url. De url dat het type cluster en hoe het te bereiken aangeeft. Wij gaan vooral werken met local om te communiceren met het lokale bestandssysteem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 14:01:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/13 14:01:37 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"wordcount\").setMaster(\"yarn\") # master = resourcemanager\n",
    "sc = SparkContext(conf=conf) # hier komen een aantal warnings -> dit is normaal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wordcount voorbeeld\n",
    "\n",
    "Om de api van pyspark te leren kennen kan je gaan naar de [documentatie](https://spark.apache.org/docs/latest/api/python/reference/index.html).\n",
    "Een eerder stap bij stap uitleg kan je [hier](https://spark.apache.org/docs/latest/api/python/getting_started/index.html) vinden.\n",
    "\n",
    "In onderstaande code gaan we stap voor stap het wordcount-voorbeeld uitwerken.\n",
    "\n",
    "Eerst moet er een pyspark context aangemaakt worden als volgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # maakt een spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# inlezen als rdd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/bigdata/Spark/input.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# lees de file en maak een rdd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m words \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# dit is niet wat we willen -> gaat eigenlijk lijsten gaan tellen (hoeveel keer komt elke lijn voor)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m words \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# dit geeft 1 lijst met alle woorden in -> hier kan een mapreduce op uitgevoerd worden\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# inlezen als rdd\n",
    "file = sc.textFile('/user/bigdata/Spark/input.txt') # lees de file en maak een rdd\n",
    "words = file.map(lambda line: line.split(' ')) # dit is niet wat we willen -> gaat eigenlijk lijsten gaan tellen (hoeveel keer komt elke lijn voor)\n",
    "words = file.flatMap(lambda line: line.split(' ')) # dit geeft 1 lijst met alle woorden in -> hier kan een mapreduce op uitgevoerd worden\n",
    "print(words.collect())\n",
    "wordcounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b: a+b) # het klassieke wordcount\n",
    "print(wordcounts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts.saveAsTextFile('/user/bigdata/Spark/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[57] at collect at /tmp/ipykernel_505/2837603738.py:5\n",
      "['Hello', 'World,', 'hello', 'world,', 'hello', 'world,', '', 'Dit', 'is', 'een', 'voorbeeld', 'file', 'om', 'het', 'Wordcount', 'voorbeeld', 'te', 'testen', '!']\n"
     ]
    }
   ],
   "source": [
    "print(words) # dit print de variabele niet uit (dit is een rdd dat in de cluster staat)\n",
    "print(words.collect()) # collect brengt de variabele lokaal (dan is het een lijst)\n",
    "# pas hier wel op dat je enkel collect op eindresultaten -> anders kan je de hele dataset downloaden wat suboptimaal is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wat gebeurt er in dit voorbeeld?**\n",
    "\n",
    "Sparkcontext om een connectie te maken met de distributed storage\n",
    "De input file wordt dan ingelezen met de textFile functie.\n",
    "Door middel van de flatMap functie wordt de tekst lijn per lijn ingelezen en gesplits in woorden. \n",
    "Dit resulteert in een RDD (Resilient Distributed Dataset.\n",
    "De .map() functie maakt een key-value pair aan voor elke keer dat het woord voorkomt.\n",
    "In een laatste fase is er een reduce stap per key die de som neemt van alle keren dat het woord voorkomt om de uiteindelijke wordcount te nemen.\n",
    "Of af te ronden wordt het resultaat opgeslagen.\n",
    "\n",
    "![spark wordcount in yarn](images/yarn_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "Nu gaan we stuk voor stuk de verschillende stappen bekijken om een pyspark applicatie te maken.\n",
    "De eerste stap is het aanmaken van een sessie (SparkSession) wat het beginpunt is voor spark applications.\n",
    "Er zijn twee manieren om een SparkSession aan te maken:\n",
    "* builder()\n",
    "* newSession()\n",
    "\n",
    "Bij het aanmaken van een session wordt er intern een SparkContext object aangemaakt. \n",
    "Dit object stelt de connectie naar een cluster voor.\n",
    "Er kan maar 1 context tegelijkertijd actief zijn.\n",
    "Als je wil connecteren met een tweede cluster moet je eerst stop() oproepen op de reeds actieve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark = SparkSession.builder.master('local[1]').appName('spark les').getOrCreate() # voer het lokaal uit met 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 14:23:04 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "spark = SparkSession.builder.config('spark.driver.cores', 2).appName('spark les').getOrCreate() # met config kan je specifieke config-parameters kiezen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "Op basis van het SparkSession object is het dan mogelijk om RDD-objecten aan te maken.\n",
    "Een RDD is de basis dataobject binnen Spark dat in parallel op verschillende nodes binnen een cluster kan uitgevoerd worden.\n",
    "Alle dataobjecten binnen spark horen tot deze klassen en dus zijn er veel mogelijkheden om RDD's aan te maken.\n",
    "Hier haal ik er twee aan:\n",
    "* parallelize() om bestaande python objecten om te zetten naar een RDD\n",
    "* textFile() of andere read methoden om bestanden op de cluster uit te lezen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = [('Student 1', 8), ('Student 2', 16), ('Student 3', 13)]\n",
    "rdd = spark.sparkContext.parallelize(dataList)\n",
    "\n",
    "rdd2 = spark.sparkContext.textFile('/user/bigdata/Spark/input.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met bovenstaande methoden hebben we twee rdd's aangemaakt. \n",
    "Op deze objecten kunnen nu verscheidene operaties uitgevoerd worden.\n",
    "Een belangrijke eigenschap van dit type objecten is dat ze steeds in parallel uitgevoerd worden.\n",
    "\n",
    "De beschikbare operaties kunnen in twee groepen verdeeld worden:\n",
    "* transformaties\n",
    "* acties\n",
    "\n",
    "[Transformaties](https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/) zijn lazy-operations waarvoor de berekening uitgesteld wordt en geven een nieuw RDD terug.\n",
    "Een aantal voorbeelden van transformaties zijn:\n",
    "* flatMap()\n",
    "* map()\n",
    "* reduceByKey()\n",
    "* filter()\n",
    "* sortByKey()\n",
    "\n",
    "[Acties](https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/) zijn operaties die een berekening starten (ook van de nodige transformaties) en geven een niet RDD-object terug. \n",
    "Een aantal voorbeelden hiervan zijn:\n",
    "* count()\n",
    "* collect()\n",
    "* first()\n",
    "* max()\n",
    "* reduce()\n",
    "\n",
    "Lees nu bovenstaande links en geef de functies die nodig zijn voor de volgende vragen op te lossen. Geef ook aan of het transformaties zijn of acties:\n",
    "* Het aantal keer dat elke waarde aanwezig is in de dataset (1 functie voor wordcount uit te voeren)\n",
    "* Uitfilteren van rijen\n",
    "* Groeperen van een aantal rijen op basis van een bepaalde waarde.\n",
    "* Toevoegen van een kolom aan elke key (bvb de lengte van een woord)\n",
    "* Hoe doe je head() uit pandas op RDD's?\n",
    "* Hoe doe je de apply() uit pandas op RDD's?\n",
    "\n",
    "Maak nu een spark applicatie dat van de eerste RDD (met de studenten) telt hoeveel studenten geslaagd zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda student: student[1] >= 10).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De applicatie voor het berekenen van een gemiddelde is iets complexer.\n",
    "Dit soort applicaties kan geschreven worden als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg 12.333333333333334\n"
     ]
    }
   ],
   "source": [
    "# hoeveel studenten zijn er?\n",
    "aantal = rdd.count()\n",
    "\n",
    "# wat is het totaal?\n",
    "totaal = rdd.map(lambda student: student[1]).reduce(lambda a, b: a+b)\n",
    "\n",
    "print('avg', totaal / aantal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf nu een mapreduce applicatie in spark om de tweede RDD van de input te verwerken en het aantal woorden van elke lengte te bekomen.\n",
    "Tussenresultaten kan je tonen om te debuggen met de .collect() functie. Dit haalt de dataset uit het gedistribueerde Spark geheugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World,', 'hello', 'world,', 'hello', 'world,', '', 'Dit', 'is', 'een', 'voorbeeld', 'file', 'om', 'het', 'Wordcount', 'voorbeeld', 'te', 'testen', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 4), (0, 1), (2, 3), (4, 1), (5, 3), (3, 3), (9, 3), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "words = rdd2.flatMap(lambda line: line.split(' ')) # splits de woorden en maak er 1 lijst van\n",
    "print(words.collect())\n",
    "result = words.map(lambda word: (len(word), 1)).reduceByKey(lambda a, b: a+b).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefeningen\n",
    "\n",
    "Maak een RDD van onderstaande lijst en voer de volgende operaties uit:\n",
    "* Maak een RDD van de bovenstaande lijst.\n",
    "* Bereken de som van de getallen in de RDD.\n",
    "* Bereken het gemiddelde van de getallen in de RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 3.0\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "sc = spark.sparkContext\n",
    "rdd_oef1 = sc.parallelize(data)\n",
    "\n",
    "print(rdd_oef1.sum(), rdd_oef1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laad het input.txt bestand in een RDD (vanuit het hdfs) en voer onderstaande berekeningen uit\n",
    "* Tel het aantal regels in het bestand.\n",
    "* Vind het aantal woorden in het bestand.\n",
    "* Maak een nieuwe RDD met alleen de regels die het woord \"world\" bevatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello world,', 'hello world,']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_oef2 = sc.textFile('/user/bigdata/Spark/input.txt')\n",
    "rdd_oef2.flatMap(lambda line: line.split(' ')).count()\n",
    "rdd_oef2.filter(lambda line: \"world\" in line).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voer volgende filter- en transformatieoperaties uit op een RDD van woorden\n",
    "* Maak een RDD van de onderstaande lijst met woorden.\n",
    "* Filter de RDD om alleen de woorden te behouden die langer zijn dan 4 letters.\n",
    "* Zet alle overgebleven woorden om naar hoofdletters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"apple\", \"banana\", \"cherry\", \"date\", \"fig\", \"grape\", \"kiwi\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gebruik verschillende acties om resultaten uit een RDD te halen.\n",
    "* Maak een RDD van de onderstaande lijst.\n",
    "* Gebruik de actie collect() om alle elementen in de RDD op te halen.\n",
    "* Gebruik de actie count() om het aantal elementen in de RDD te tellen.\n",
    "* Gebruik de actie reduceByKey() om de totale waarde per naam te berekenen (als je dezelfde naam meerdere keren hebt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3), (\"David\", 4), (\"Eve\", 5)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groepeer data en bereken samenvattingen door volgende taken uit te voeren:\n",
    "* Maak een RDD van onderstaande lijst.\n",
    "* Groepeer de items op basis van hun type (fruit/vegetable).\n",
    "* Maak een RDD die het aantal items per type toont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vegetable', 2), ('fruit', 4)]\n"
     ]
    }
   ],
   "source": [
    "data = [(\"fruit\", \"apple\"), (\"fruit\", \"banana\"), (\"vegetable\", \"carrot\"), \n",
    "        (\"fruit\", \"orange\"), (\"vegetable\", \"celery\"), (\"fruit\", \"kiwi\")]\n",
    "\n",
    "rdd_oef5 = sc.parallelize(data)\n",
    "\n",
    "grouped = rdd_oef5.groupByKey()\n",
    "counted = grouped.mapValues(list).mapValues(len)\n",
    "print(counted.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes\n",
    "\n",
    "Een belangrijke subklasse van RDD's zijn dataframes.\n",
    "Dit is een veel gebruikte manier om gestructureerde data voor te stellen.\n",
    "Dataframes in spark is sterk gerelateerd aan de dataframes gezien in pandas.\n",
    "Het belangrijskte verschil is dat ze verdeeld worden over de cluster en operaties op de dataframes in parallel uitgevoerd worden.\n",
    "Dataframes kunnen aangemaakt worden door gebruik te maken van de createDataFrame functie in context of ingelezen worden vanuit csv's of jsons. Ten slotte kunnen dataframes ook komen van externe bronnen zoals databases als resultaat van een sql-query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+--------+---------+\n",
      "| Voornaam|   Naam|Geboortedatum|Geslacht|   Budget|\n",
      "+---------+-------+-------------+--------+---------+\n",
      "|    Harry| Potter|   1980-07-31|       M|100000000|\n",
      "|   Ronald|  Wemel|   1980-04-01|       M|       10|\n",
      "|Hermelijn|Griffel|   1979-09-19|       F|     4000|\n",
      "+---------+-------+-------------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 11:47:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 12:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+-------------+--------+-------------------+\n",
      "|summary|Voornaam|   Naam|Geboortedatum|Geslacht|             Budget|\n",
      "+-------+--------+-------+-------------+--------+-------------------+\n",
      "|  count|       3|      3|            3|       3|                  3|\n",
      "|   mean|    NULL|   NULL|         NULL|    NULL|         3.333467E7|\n",
      "| stddev|    NULL|   NULL|         NULL|    NULL|5.773386936614157E7|\n",
      "|    min|   Harry|Griffel|   1979-09-19|       F|                 10|\n",
      "|    max|  Ronald|  Wemel|   1980-07-31|       M|          100000000|\n",
      "+-------+--------+-------+-------------+--------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [('Harry', 'Potter','1980-07-31','M',100000000),\n",
    "  ('Ronald','Wemel','1980-04-01','M',10),\n",
    "  ('Hermelijn','Griffel','1979-09-19','F',4000)\n",
    "]\n",
    "\n",
    "columns = ['Voornaam', 'Naam', 'Geboortedatum', 'Geslacht', 'Budget']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()\n",
    "df.describe() # describe is een transformatie geen actie -> geen output getoond\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark SQL\n",
    "\n",
    "Bovenstaande datastructuren (RDD's en Dataframes) zijn een onderdeel van het Pyspark sql module.\n",
    "De Spark API heeft een hele reeks methoden en functies om deze in te laden, uit te lezen en te manipuleren.\n",
    "Daarnaast maakt deze module het ook mogelijk om SQL-queries uit te voeren op dataframes.\n",
    "Om SQL-queries uit te voeren op dataframes moet er eerst een view gemaakt worden in het dataframe met de functie createOrReplaceTempView(\"view_name\")\n",
    "\n",
    "Daarna kan je gebruik maken van de .sql() functie om allerhande sql queries uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------------+--------+---------+\n",
      "|Voornaam|  Naam|Geboortedatum|Geslacht|   Budget|\n",
      "+--------+------+-------------+--------+---------+\n",
      "|   Harry|Potter|   1980-07-31|       M|100000000|\n",
      "|  Ronald| Wemel|   1980-04-01|       M|       10|\n",
      "+--------+------+-------------+--------+---------+\n",
      "\n",
      "+--------+------+-------------+--------+---------+\n",
      "|Voornaam|  Naam|Geboortedatum|Geslacht|   Budget|\n",
      "+--------+------+-------------+--------+---------+\n",
      "|   Harry|Potter|   1980-07-31|       M|100000000|\n",
      "|  Ronald| Wemel|   1980-04-01|       M|       10|\n",
      "+--------+------+-------------+--------+---------+\n",
      "\n",
      "+--------+--------+\n",
      "|Geslacht|count(1)|\n",
      "+--------+--------+\n",
      "|       M|       2|\n",
      "|       F|       1|\n",
      "+--------+--------+\n",
      "\n",
      "+--------+-----+\n",
      "|Geslacht|count|\n",
      "+--------+-----+\n",
      "|       M|    2|\n",
      "|       F|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecteer alle rijen van mannelijke karakters\n",
    "df.createOrReplaceTempView('test')\n",
    "df_male = spark.sql(\"select * from test where Geslacht == 'M'\")\n",
    "df_male.show()\n",
    "\n",
    "df_male2 = df.filter(df.Geslacht == 'M')  # in pandas was het df[df.Geslacht == 'M']\n",
    "df_male2.show()\n",
    "\n",
    "# group by\n",
    "spark.sql('select Geslacht, count(*) from test group by Geslacht').show()\n",
    "df.groupby('Geslacht').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buiten de functionaliteit om SQL queries uit te voeren is ook het lezen en schrijven van allerhande dataformaten een belangrijk onderdeel van de pyspark sql module.\n",
    "Meer informatie hierover kun je [hier](https://spark.apache.org/docs/latest/sql-data-sources.html) vinden in de documentatie.\n",
    "In essentie ziet de code er uit als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+-----------------+----------------+------+\n",
      "|_c0|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|\n",
      "+---+-----------------+----------------+-----------------+----------------+------+\n",
      "|  0|              5.1|             3.5|              1.4|             0.2|     0|\n",
      "|  1|              4.9|             3.0|              1.4|             0.2|     0|\n",
      "|  2|              4.7|             3.2|              1.3|             0.2|     0|\n",
      "|  3|              4.6|             3.1|              1.5|             0.2|     0|\n",
      "|  4|              5.0|             3.6|              1.4|             0.2|     0|\n",
      "|  5|              5.4|             3.9|              1.7|             0.4|     0|\n",
      "|  6|              4.6|             3.4|              1.4|             0.3|     0|\n",
      "|  7|              5.0|             3.4|              1.5|             0.2|     0|\n",
      "|  8|              4.4|             2.9|              1.4|             0.2|     0|\n",
      "|  9|              4.9|             3.1|              1.5|             0.1|     0|\n",
      "| 10|              5.4|             3.7|              1.5|             0.2|     0|\n",
      "| 11|              4.8|             3.4|              1.6|             0.2|     0|\n",
      "| 12|              4.8|             3.0|              1.4|             0.1|     0|\n",
      "| 13|              4.3|             3.0|              1.1|             0.1|     0|\n",
      "| 14|              5.8|             4.0|              1.2|             0.2|     0|\n",
      "| 15|              5.7|             4.4|              1.5|             0.4|     0|\n",
      "| 16|              5.4|             3.9|              1.3|             0.4|     0|\n",
      "| 17|              5.1|             3.5|              1.4|             0.3|     0|\n",
      "| 18|              5.7|             3.8|              1.7|             0.3|     0|\n",
      "| 19|              5.1|             3.8|              1.5|             0.3|     0|\n",
      "+---+-----------------+----------------+-----------------+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 11:57:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , sepal length (cm), sepal width (cm), petal length (cm), petal width (cm), target\n",
      " Schema: _c0, sepal length (cm), sepal width (cm), petal length (cm), petal width (cm), target\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://namenode:9000/user/bigdata/Spark/input.csv\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', True).option('delimiter', ',').csv('/user/bigdata/Spark/input.csv').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De opties die hierbij gekozen kunnen worden kun je vinden in de documentatie.\n",
    "\n",
    "Daarnaast zijn er ook functionaliteiten om data uit te lezen speciaal voor Machine Learning zoals libsvm en image-directories maar die worden later getoond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Net zoals RDD kunnen er een aantal operaties uitgevoerd worden op deze dataframes.\n",
    "Een volledige lijst met alle operaties kan je [hier](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis) vinden.\n",
    "Zoek de functies die gebruikt moeten worden om de volgende zaken uit te voeren:\n",
    "* Groepeer volgens een bepaalde sleutel\n",
    "* Krijg een lijst met alle kolomnamen\n",
    "* Filter rijen uit\n",
    "* Verwijder null-values in de dataset via rijen\n",
    "* Verwijder null-values door kolommen te verwijderen\n",
    "* Bereken een dataframe met statistieken van het dataframe\n",
    "* Krijg een dataframe met alle nan waarden\n",
    "* Hoe krijg je informatie zoals .info()\n",
    "* Hoe werkt het groeperen van informatie op basis van een key/kolom\n",
    "\n",
    "Probeer deze ook uit op bovenstaand aangemaakt dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lees daarna volgende [link](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/) om een idee te krijgen over hoe verschillende functies uit te voeren op deze dataframes.\n",
    "Werk nu de volgende oefening uit en maak hiervoor een spark applicatie:\n",
    "* Schrijf de code om de input.csv uit te lezen uit het hdfs en om te zetten naar een dataframe\n",
    "* Print het dataschema uit voor het dataframe, hoeveel kolommen zijn er aanwezig in het dataframe.\n",
    "* Bereken het minimum en maximum van de 'sepal width (cm)' en 'petal width (cm)' kolom.\n",
    "* Hernoem de target kolom naar label\n",
    "* Hernoem de labels 0 naar Soort 0, labels 1 naar Soort 2 en labels 3 naar Soort 3\n",
    "* Voer normalisatie van de eerste 4 kolommen uit (het gemiddelde ervan aftrekken en delen door de standaardafwijking)\n",
    "* Controleer de voorgaande stap door opnieuw het gemiddelde en de standaardafwijking te berekenen. Deze moeten respectievelijk 0 en 1 zijn.\n",
    "* Bereken de oppervlakte van sepal door de lengte en breedte ervan te vermenigvuldigen. Noem deze nieuwe kolom sepal area. Doe dit ook voor de petal.\n",
    "* Groepeer nu de rijen op de label kolom. Bereken per groep het gemiddelde van elke kolom. Is er een verschil tussen de verschillende klassen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared variabelen**\n",
    "\n",
    "Variabelen met read-write acces zijn zeer inefficient om te gebruiken in een cluster met sterke parallelisatie.\n",
    "Spark bied echter twee varianten aan die wel efficient geimplementeerd kunnen worden, namelijk\n",
    "* Broadcasted variabelen\n",
    "* Accumulators\n",
    "\n",
    "Broadcasted variabelen zijn read-only variabelen, die aangemaakt worden door de driver en eenmalig verspreid worden over de nodes in plaats van voor elke job.\n",
    "Dit wordt vooral gebruikt om grote data die veelvuldig gebruikt wordt te cachen op de nodes.\n",
    "Bij het gebruik van broadcast variabelen is het belangrijk om te onthouden dat je de originele variabele niet meer mag gebruiken na het aanmaken van de broadcasted variabele omdat ze anders toch nog elke job doorgestuurd wordt.\n",
    "De belangrijkste functies om te werken met broadcasted variabelen zijn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accumulators**\n",
    "\n",
    "Het andere type dat aangeboden wordt zijn accumulators.\n",
    "Deze laten enkel toe dat noden iets toevoegen aan een gedeelde variabele.\n",
    "Enkel de driver kan deze variabele uitlezen.\n",
    "Dit kan bijvoorbeeld gebruikt worden om tellers of sommen bij te houden.\n",
    "Deze accumulators kunnen een naam hebben (named accumulators zijn zichtbaar in de wep api).\n",
    "De ingebouwde accumulator van Spark ondersteunt enkel numerieke accumulators.\n",
    "Het is echter mogelijk om eigen accumulators toe te voegen door over te erven van de AccumulatorParam klasse en deze twee functies te implementeren:\n",
    "* zero: De begin waarde van de accumulator\n",
    "* addInPlace: Om twee waarden samen te voegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefeningen\n",
    "\n",
    "Maak een pyspark DataFrame van onderstaande lijst met dictionaries en voer volgende operaties uit:\n",
    "* Toon de schema van het DataFrame.\n",
    "* Filter het DataFrame om alleen de rijen te behouden waar de leeftijd groter is dan 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
    "    {\"name\": \"Cathy\", \"age\": 28, \"city\": \"Chicago\"},\n",
    "    {\"name\": \"David\", \"age\": 35, \"city\": \"New York\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laad het input.csv CSV-bestand in een pyspark DataFrame en voer volgende operaties uit:\n",
    "* Toon de eerste 3 rijen van het DataFrame.\n",
    "* Bereken de gemiddelde sepal length in het DataFrame.\n",
    "* Groepeer het DataFrame op basis van het target en tel het aantal per klasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voer met onderstaande data de volgende stappen uit:\n",
    "* Maak een pyspark DataFrame van de onderstaande lijst.\n",
    "* Voeg een nieuwe kolom age_after_5_years toe die de leeftijd over 5 jaar toont.\n",
    "* Filter het DataFrame om alleen de rijen te behouden waar de leeftijd na 5 jaar groter is dan 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30},\n",
    "    {\"name\": \"Bob\", \"age\": 25},\n",
    "    {\"name\": \"Cathy\", \"age\": 28},\n",
    "    {\"name\": \"David\", \"age\": 35},\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gebruik acties om de volgende resultaten uit het DataFrame van de vorige stap te halen door de volgende stapen te implementeren\n",
    "* Gebruik de actie count() om het aantal rijen in het DataFrame te tellen.\n",
    "* Gebruik de actie show() om de inhoud van het DataFrame te tonen.\n",
    "* Selecteer alleen de name en age kolommen en toon de resultaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voer de volgende aggregaties uit op een pyspark DataFrame op basis van onderstaande data:\n",
    "* Groepeer het DataFrame op basis van de afdeling en bereken het gemiddelde salaris per afdeling.\n",
    "* Sorteer de resultaten op basis van het gemiddelde salaris in aflopende volgorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"department\": \"Sales\", \"employee\": \"Alice\", \"salary\": 70000},\n",
    "    {\"department\": \"Sales\", \"employee\": \"Bob\", \"salary\": 60000},\n",
    "    {\"department\": \"HR\", \"employee\": \"Cathy\", \"salary\": 80000},\n",
    "    {\"department\": \"HR\", \"employee\": \"David\", \"salary\": 75000},\n",
    "    {\"department\": \"IT\", \"employee\": \"Eve\", \"salary\": 90000},\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Door de hoge populariteit van pandas in python is er een alternatief uitgewerkt binnen de laatste versie van Spark (eind 2021) dat de pandas api integreert.\n",
    "Hierdoor kan je code schrijven die identiek is aan te werken met pandas.\n",
    "Meer informatie over deze api vind je op de [pyspark documentatie](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html).\n",
    "\n",
    "De volgende twee delen van deze documentatie zijn belangrijk:\n",
    "* [De beschikbare pandas functies](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/supported_pandas_api.html): Niet alle functies van pandas kunnen gebruikt worden. Dit komt doordat pandas geschreven is voor data die volledig lokaal ingeladen is in het geheugen. Omdat dit gevaarlijk kan zijn in het geval van grote datasets zijn de functies die dit zouden uitvoeren niet geimplementeerd. De meeste functies die we gebruiken zijn gelukkig wel geimplementeerd.\n",
    "* [De best practices](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html) om te werken met pandas on spark. Samengevat is het:\n",
    "  * Gebruik de .explain() functie in spark om het plan van uitvoering te bekijken indien het uitvoeren te lang duurt\n",
    "  * Gebruik checkpoints voor de efficientie van de planner van het stappenplan (spark.checkpoint() of spark.local_checkpoint()\n",
    "  * **Vermijd shuffling van data zoals sort-operaties**. Hierbij wordt data tussen nodes verstuurd wat kan resulteren in heel wat communicatie tussen verschillende nodes\n",
    "  * **Pas op met kolomnamen**: Gebruik geen duplicaten en vermijd gereserveerde namen met leading en trailing dubbele underscores\n",
    "  * **Stel waar mogelijk de index kolom in bij converteren naar een pandas-on-spark dataframe**: Veel functies werken efficienter bij het gebruik van een goede indexwaarde ipv de default.\n",
    "  * **Vermijd merge/join operaties waar mogelijk**: Ook deze operaties vereisen heel wat communicatie dus worden best vermeden\n",
    "  * Gebruik zoveel mogelijk van de pandas-on-spark API direct ipv standaard python functies om conflicten te vermijden\n",
    "     * df.sum() werkt gedistribueerd maar sum(df) niet wat tot fouten en errors leidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# pandas dataframe\n",
    "df = load_iris(as_frame=True).frame\n",
    "display(df)\n",
    "\n",
    "# pandas on spark\n",
    "df_pandas_on_spark = ps.from_pandas(df)\n",
    "df_pandas_on_spark\n",
    "\n",
    "# pandas on spark to pandas\n",
    "df2 = df_pandas_on_spark.to_pandas()\n",
    "display(df2)\n",
    "\n",
    "# pandas on spark to spark dataframe\n",
    "spark_dataframe = df_pandas_on_spark.to_spark()\n",
    "spark_dataframe.show()\n",
    "\n",
    "# back to pandas on spark dataframe\n",
    "df_pandas_on_spark2 = spark_dataframe.pandas_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maak een pyspark DataFrame van onderstaande lijst met dictionaries en voer volgende operaties uit met pandas-on-spark:\n",
    "* Toon de schema van het DataFrame.\n",
    "* Filter het DataFrame om alleen de rijen te behouden waar de leeftijd groter is dan 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
    "    {\"name\": \"Cathy\", \"age\": 28, \"city\": \"Chicago\"},\n",
    "    {\"name\": \"David\", \"age\": 35, \"city\": \"New York\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laad het input.csv CSV-bestand in een pyspark DataFrame en voer volgende operaties uit met pandas-on-spark:\n",
    "* Toon de eerste 3 rijen van het DataFrame.\n",
    "* Bereken de gemiddelde sepal length in het DataFrame.\n",
    "* Groepeer het DataFrame op basis van het target en tel het aantal per klasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maak een pandas-on-Spark DataFrame op basis van onderstaande gegevens.\n",
    "\n",
    "Voeg  daarna een nieuwe kolom age_after_5_years toe die de leeftijd over 5 jaar toont.\n",
    "\n",
    "Filter het DataFrame om alleen de rijen te behouden waar de leeftijd na 5 jaar groter is dan 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Cathy\", \"David\"],\n",
    "    \"age\": [30, 25, 28, 35],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verdere oefeningen\n",
    "\n",
    "Gebruik nu de titanic csv om met behulp van een spark applicatie de volgende zaken te berekenen:\n",
    "* Het aantal unieke plaatsen waar personen aan boord zijn gekomen (embarked kolom)\n",
    "* Het aantal ontbrekende waarden in de Cabin kolom\n",
    "* De volgende statistische waarden voor de ticketprijs (Fare) kolom: min, max, mean, std\n",
    "* De langste naam van een passagier\n",
    "* Het aantal passagiers per klasse\n",
    "* Het totaal aantal passagiers op de titanic\n",
    "\n",
    "Het is de bedoeling dat je elk element afzonderlijk berekend dus je moet geen dataframe uitkomen waar al deze zaken in zitten. \n",
    "\n",
    "Tip: herstart je kernel zodat je het opzetten van de sparkcontext ook oefent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
