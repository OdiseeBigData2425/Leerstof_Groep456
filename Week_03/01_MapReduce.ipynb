{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce\n",
    "\n",
    "Eerst maken we een aparte directory aan voor alles wat we voor deze notebook gaan gebruiken in hdfs. Dit om conflicten of het overschrijven van gegevens te vermijden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "map = 'MapReduce'\n",
    "\n",
    "client = InsecureClient('http://localhost:9870', user='bigdata')\n",
    "\n",
    "if client.status(map, strict=False) is None:\n",
    "    client.makedirs(map)\n",
    "else:\n",
    "    # do some cleaning in case anything else than *.txt is present\n",
    "    for f in client.list(map):\n",
    "        client.delete(map + '/' + f, recursive=True)\n",
    "\n",
    "client.upload(map, 'input.txt')\n",
    "client.upload(map, 'titanic.csv')\n",
    "with client.read(map + '/input.txt') as reader:\n",
    "    content = reader.read()\n",
    "    print(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wat is MapReduce\n",
    "\n",
    "MapReduce is een programmeermodel om eenvoudig distributed data te verwerken.\n",
    "Het is belangrijk om te realiseren dat de programma's die je hier schrijft een parallel uitgevoerd worden op verschillende stukjes data (De map-fase) om daarna in de reduce-fase tot een finale output teruggebracht te worden.\n",
    "Er gebeuren 5 stappen bij het uitvoeren van een MapReduce programma\n",
    "* Bepalen op welke nodes de code uitgevoerd wordt (wordt door YARN gedaan afhankelijk van de locatie van de blocks)\n",
    "* Uitvoeren van de Map-code (Geschreven door de developer naar eigen wens)\n",
    "* Shuffle, ouput van de map-fase doorsturen naar andere nodes die de resultaten gaan reduceren (Automatisch)\n",
    "* uitvoeren van de Reduce-code (Geschreven door de developer naar eigen wens)\n",
    "* Combineren van de reduce output tot 1 gehele/finale output (Automatisch)\n",
    "\n",
    "Uit bovenstaand stappenplan is het duidelijk dat er twee zaken moeten geimplementeerd worden bij het schrijven van een MapReduce toepassing.\n",
    "Echter zullen we eerst een aantal voorbeelden bestuderen met reeds bestaande implementaties om zo meer vertrouwd te geraken met de flow van MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Voorbeelden van bestaande applicaties\n",
    "\n",
    "Reeds een aantal default MapReduce applications zijn mee geinstalleerd met Hadoop.\n",
    "De jar die deze toepassingen bevat kan gevonden worden in hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar.\n",
    "Wanneer je deze jar uitvoert met onderstaande commando krijg je een lijst met de beschikbare applicaties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!hadoop jar hadoop-mapreduce-examples-3.3.6.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In deze notebook gaan we vooral focussen op het typische probleem van wordcount.\n",
    "Dit is een toepassing dat gaat tellen hoe vaak elk woord voorkomt in een bepaalde tekst.\n",
    "Om meer informatie over deze toepassing te krijgen kan je gebruik maken van het volgende commando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!hadoop jar hadoop-mapreduce-examples-3.3.6.jar wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In de eerste cell hebben we het input.txt bestand geupload. Of dit correct gebeurd is kan je controleren op de [file explorer van het hdfs](http://localhost:9870/explorer.html#/user/bigdata)\n",
    "\n",
    "Met onderstaande commando kan nu het precompiled word count example uitgevoerd worden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!hadoop jar hadoop-mapreduce-examples-3.3.6.jar wordcount /user/bigdata/MapReduce/input.txt /user/bigdata/MapReduce/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Na het uitvoeren van het wordcount applicatie is er op het HDFS een extra folder toegevoegd met als naam **output**.\n",
    "De plaats waar de output bewaard wordt is opgegeven in het bovenstaande commando en kan bekeken worden via de file-explorer van het hdfs.\n",
    "Deze folder bevat de volgende files:\n",
    "* Een _SUCCESS file dat leeg is. Deze file wordt gebruikt om aan te geven dat de MapReduce applicatie die resulteerde in de output goed afgerond was.\n",
    "* Een part-r-00000 file dat de output bevat. Indien de output te groot is kan het zijn dat deze file in meerdere files gesplitst wordt. Hier is dit niet het geval en bevat deze file alle ouput. De output van het bovenstaande commando zijn key-value paren waar de keys de verschillende woorden zijn en de values hoeveel keer elk woord voorkomt.\n",
    "\n",
    "De output van het commando kan dan met onderstaande code uitgelezen worden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with client.read(map + '/output/part-r-00000') as reader:\n",
    "    content = reader.read()\n",
    "    print(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs/process monitoring\n",
    "\n",
    "Bovestaande wordcount applicatie wordt uitgevoerd op YARN op de verschillende nodes van de cluster. Dit houdt in dat YARN gebruikt kan worden om de status van de applicatie op te volgen. Hiervoork kan je server naar [de webpagina van Yarn](http://localhost:8088). Na het klikken op de juiste status krijg je de correcte applicatie te zien. Hier kan je ook de logs bekijken door op logs te klikken.\n",
    "\n",
    "**LET OP: Doordat we in de webbrowser buiten de docker-containers zitten, kunnen we niet de historyserver hostname gebruiken. Als je een DNS-lookup error krijgt, vervang dan de domeinnaam/hostname door localhost.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## OEFENING: MAPREDUCE \n",
    "\n",
    "Probeer nu op basis van bovenstaande reeds bestaande applicaties de gemiddelde lengte van de woorden in de text te bepalen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# uit te voeren op commando voor de gemiddelde lengte van de woorden te bekomen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "De resource manager houdt ook een overzicht bij van de uitgevoerde applicaties, hun status, runtime en eventuele loggings. Na bovenstaande commando's uit te voeren moet je iets gelijkaardigs zien als in de onderstaande screenshot.\n",
    "\n",
    "![yarn of mapreduce](images/yarn_001.png)\n",
    "\n",
    "## Zelf implementeren van MapReduce applicaties\n",
    "\n",
    "Natuurlijk zijn er veel meer zaken mogelijk om te berekenen met map-reduce toepassingen dan de reeds gecompileerde in hadoop.\n",
    "Zoals eerder aangehaald valt vooral het coderen van de Map- en Reducestap hierbij op de schouders van de developer.\n",
    "De standaard programmeertaal van MapReduce is Java en dus ook het grootste deel van de documentatie over MapReduce is geschreven met behulp van Java.\n",
    "Deze programmas moeten dan gecompileerd worden tot een jar dat geupload kan worden naar de overeenkomstige nodes en daar uitgevoerd.\n",
    "De api overview van hadoop kan je [hier](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/package-summary.html) vinden.\n",
    "\n",
    "De code voor het wordcount example ziet er als volgt uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%file WordCount.java\n",
    "// dit schrijft onderstaande cell naar een file in de huidige directory\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Deze code bevat drie delen:\n",
    "* De main() functie: verzorgt de configuratie van de uit te voeren taak. Geeft aan wat de Map en Reduce klassen zijn, wat de input is, hoe de output bewaard wordt ,...\n",
    "* De Map-klasse met de map() functie bevat de code voor de mapping-fase\n",
    "* De Reduce-klasse met de reduce() functie bevat de code voor de reduce-fase\n",
    "\n",
    "De laatste twee klassen zijn hier gecodeerd als geneste klassen. Deze hadden ook in aparte files geplaatst kunnen worden.\n",
    "Nu moet deze code eerst omgezet/gecompileerd worden tot een .jar file. Deze kan dan analoog uitgevoerd worden als hierboven met het voorbeeldcode.\n",
    "Deze twee stappen kunnen uitgevoerd worden door onderstaande commando's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compileren tot .jar\n",
    "!javac -cp \"hadoop-common-3.3.6.jar:hadoop-mapreduce-client-core-3.3.6.jar\" -d . WordCount.java\n",
    "!jar cvf wordcounter.jar *.class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# execute\n",
    "!hadoop jar wordcounter.jar WordCount /user/bigdata/MapReduce/input.txt /user/bigdata/MapReduce/output_java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implementeren via Python\n",
    "\n",
    "Hoewel het Hadoop Ecosysteem geprogrammeerd is in Java en het dus er goed mee samenwerkt, is het niet verplicht om Java te gebruiken om MapReduce applicaties te schrijven.\n",
    "Java krijgt namelijk veel kritiek, vooral doordat er veel code nodig is om eenvoudige zaken te programmeren.\n",
    "Om andere programmeertalen te gebruiken worden er verscheidene API's aangeboden door Hadoop, namelijk\n",
    "* Hadoop Streaming\n",
    "    * Communicatie via stdin/stdout\n",
    "    * Gebruikt door hadoopy, mrjob, ...\n",
    "* Hadoop Pipes\n",
    "    * C++ interface voor Hadoop\n",
    "    * Communicatie via sockets\n",
    "    * Gebruikt door pydoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Mrjob\n",
    "\n",
    "De eerste API die we bekijken is MrJobs.\n",
    "Deze API maakt gebruik van de Hadoop Streaming API.\n",
    "De voordelen van MrJobs zijn:\n",
    "* Uitgebreidde documentatie\n",
    "* Code kan lokaal uitgevoerd worden als test\n",
    "* Data serialisatie gebeurt automatisch (nadeel van Streaming API)\n",
    "* Werkt met Amazon Elastic MapReduce en Google Cloud Dataproc\n",
    "\n",
    "Het grootste nadeel is dat de StreamingAPI niet de volledige kracht heeft van het Hadoop ecosysteem omdat alles omgezet wordt naar strings (jsons)\n",
    "\n",
    "Installatie van deze package gebeurt als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%file wordcount_mrjob.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze code kan uitgevoerd worden door het commando hieronder.\n",
    "Er zijn twee belangrijke parameters om toe te voegen aan het commando.\n",
    "* -r: Deze parameter geeft aan welke file uitgelezen wordt. Dit kan een lokale file zijn of een file op het hdfs. Om een hdfs-file aan te spreken gebruik je de volgende structuur: hdfs:///{path to file}\n",
    "* -o: Deze parameter geeft aan waar de data moet bewaard worden. Als deze parameter ontbreekt wordt het in de stdout geprint. Met de parameter kan je aangeven in welke file (lokaal of hdfs) de output moet bewaard worden.\n",
    "\n",
    "Meer informatie over hoe mrjob applicaties gestart kunnen worden vind je in [de documentatie](https://mrjob.readthedocs.io/en/latest/guides/quickstart.html#running-your-job-different-ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De bovenstaande manier vereist echter dat je steeds een python-file aanmaakt die dan via commandline gestart wordt.\n",
    "De reden hiervoor is is dat **de file naar de cluster verstuurd wordt**.\n",
    "Om de output te bekijken kan het output.txt bestand gedownload en uitgeprint worden als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with client.read(map + '/output_txt/part-00000') as reader:\n",
    "    content = reader.read()\n",
    "    print(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefeningen\n",
    "\n",
    "Maak de nodige mapreduce applicaties voor de volgende zaken te berekenen (je moet de output niet bewaren in een file):\n",
    "* De gemiddelde woordlengte\n",
    "* Het aantal keer dat elk karakter voorkomt\n",
    "* Het aantal woorden dat begint met elke letter\n",
    "* Het aantal woorden in de tekst\n",
    "\n",
    "Als een applicatie crasht, kan het zijn dat ze op de cluster nog een hele tijd actief blijft en het starten van nieuwe applicaties kan tegenhouden.\n",
    "Met onderstaande commando kan je een bestande applicatie afsluiten/killen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# killen van een applicatie\n",
    "yarn application -kill <Application_ID>\n",
    "# appliation id kan je vinden via de webbrowser van yarn (localhost:8088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file vraag1.py\n",
    "# vraag 1: gemiddelde woordlengte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test vraag 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file vraag2.py\n",
    "# vraag 2: het aantal keer dat elk karakter voorkomt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test vraag 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file vraag3.py\n",
    "# vraag 3: * Het aantal woorden dat begint met elke letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test vraag 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file vraag4.py\n",
    "# vraag 4: Het aantal woorden in de tekst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test vraag 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Werken met MapReduce voor gestructureerde data\n",
    "\n",
    "Mapreduce kan ook gebruikt worden om te werken met gestructureerde data (bijvoorbeeld een csv) waar elke rij 1 data-element voorsteld.\n",
    "Het is hier echter wel belangrijk dat alle data op 1 lijn gecombineerd wordt dus multiline csv's, jsons of xml bestanden kunnen niet verwerkt worden.\n",
    "\n",
    "Onderstaande code is een voorbeeld voor hoe je een csv kan uitlezen en een aantal statistieken kan berekenen. Hierin leer je vooral:\n",
    "* Hoe de csv lijn per lijn te verwerken en kolommen te detecteren\n",
    "* Hoe de header rij eruit filteren\n",
    "* Hoe meerdere berekeningen op een iterator te doen\n",
    "\n",
    "We gebruiken hiervoor de titanic.csv file. Let op dat die naam van de passagier hierbij komma's kan bevatten dus dit vereist wat extra aandacht.\n",
    "Daarnaast gebruiken we de lokale versie van het bestand en niet de geuploadde versie.\n",
    "\n",
    "We willen de volgende zaken berekenen door middel van 1 map-reduce applicatie:\n",
    "* Gemiddelde leeftijd\n",
    "* Percentage overleeft\n",
    "* Percentage mannelijke passagiers\n",
    "* Percentage vrouwelijke passagiers die het overleefd hebben: 30% betekend dat 30% van de vrouwelijke passagiers het overleefd hebben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file structured.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "\n",
    "Indien er een python script voor een map-reduce applicatie een error bevat is het niet altijd eenvoudig om de specifieke foutmelding te vinden. Dit is vooral merkbaar als we een runtime errors.\n",
    "Indien we in het voorgaande script de beveiligingen rond het casten van empty strings verwijderen. Dan krijgen we foutmeldingen. Indien we ditzelfde nu uitvoeren (maar uitvoeren op de cluster in plaats van lokaal in tegenstelling tot het voorgaande commando), dan krijgen we geen duidelijke foutmelding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file structured_error.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na het uitvoeren van bovenstaande script, dan zie je dat de applicatie gestart wordt (map 0% reduce 0%) maar een duideljke foutmelding is er niet (run time exception).\n",
    "Om de logs te bekijken kan je \n",
    "* surfen gaan naar de historyserver die beschikbaar is op de url: localhost:8188\n",
    "* het volgende command-line commando uitvoeren. Dit haalt alle std-err logs van alle containers op. Zoek naar een python error en je zal vinden wat er misgaat.\n",
    "* een python script schrijven om de yarn log-server te bevragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yarn logs -applicationId {app id} | sed -n '/LogType:stderr/,/End of LogType:stderr/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afsluitende opmerkingen\n",
    "\n",
    "Het is belangrijk om te beseffen dat deze applicatie op meerdere nodes kan tegelijkertijd uitgevoerd worden. \n",
    "Dit heeft als gevolg dat je geen persistente counters kan toevoegen in de reducer om gemiddelden en dergelijke te berekenen.\n",
    "Een globaal overzicht van het dataframe kan maar **in de mapper** behaald worden.\n",
    "\n",
    "Daarnaast is er nog een andere stap mogelijk dan mapper of reducer. Dat is **de combiner** stap. Dit is een stap die runt per node en gebruikt kan worden om al wat combinaties te doen zodat er minder data tussen nodes moet verstuurd worden. In grote applicaties/datasets kan dit heel wat internettrafiek besparen wat de werking van de cluster ten goede komt.\n",
    "\n",
    "Ten derde is het ook mogelijk om meerdere stappen te definieren in de mrjob applicatie. Dit kan door de steps functie te implementeren en daar elke stap in te definieren. Elke stap hierin kan beschikken over een eigen mapper/reducer en combiner functie. Meer informatie vind je [in de documentatie](https://mrjob.readthedocs.io/en/latest/guides/quickstart.html#writing-your-second-job)\n",
    "\n",
    "## Verdere oefeningen\n",
    "\n",
    "Gebruik nu de titanic csv om met behulp van een mapreduce applicatie de volgende zaken te berekenen:\n",
    "* Het aantal unieke plaatsen waar personen aan boord zijn gekomen (embarked kolom)\n",
    "* Het aantal ontbrekende waarden in de Cabin kolom\n",
    "* De volgende statistische waarden voor de ticketprijs (Fare) kolom: min, max, mean, std\n",
    "* De langste naam van een passagier\n",
    "* Het aantal passagiers per klasse\n",
    "* Het totaal aantal passagiers op de titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file structured_oefening.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
