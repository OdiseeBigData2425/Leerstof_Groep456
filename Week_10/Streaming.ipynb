{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Streaming\n",
    "\n",
    "Voorheen hebben we gemerkt met een bestaande, volledige dataset die reeds aanwezig was in de cluster.\n",
    "Echter is er vaak een nood aan data binnen te halen van verscheidene databronnen, deze om te zetten naar een bruikbaar formaat en te bewaren in een datawarehouse.\n",
    "Dit is exact wat er nodig is voor het ETL principe of Extract-Transform-Load.\n",
    "Een belangrijk onderdeel hiervan is de Pyspark Streaming module.\n",
    "De documentatie van deze module kan [hier](https://spark.apache.org/docs/latest/streaming-programming-guide.html) gevonden worden.\n",
    "Het wordcount example kan ook hierin geschreven worden, namelijk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting networkwordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file networkwordcount.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "# tip: werk met scripts die in bestanden gaan geplaatst worden indien je werkt met streaming\n",
    "# maakt het makkelijker om streams te stoppen\n",
    "# deze bestanden moeten standalone zijn: maak de sparkcontext opnieuw aan + zorg voor de nodige imports\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 5 second\n",
    "sc = SparkContext(\"local[2]\", \"networkwordcount\")\n",
    "sc.setLogLevel(\"ERROR\") # reduce spam of logging\n",
    "ssc = StreamingContext(sc, 5) # om de 5 seconden\n",
    "\n",
    "lines = ssc.socketTextStream('localhost', 19999) # de poort waarop deze applicatie luistert naar binnenkomende berichten\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "pairs = words.map(lambda x: (x,1))\n",
    "counts = pairs.reduceByKey(lambda x,y : x+y)\n",
    "\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination() # zorg ervoor dat de applicatie actief blijft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om nu de werking te starten, open in jupyterlab een terminal.\n",
    "Dit is een tool die ons toelaat om tekst te vesturen naar een locale netwerkpoort om zo een tekststroom na te bootsen.\n",
    "Voer daarna het volgende commando uit:\n",
    "```console\n",
    "    nc -lk 19999\n",
    "```\n",
    "\n",
    "Het wordcount programma kan nu gestart worden door het volgende commando uit te voeren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/20 12:09:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:05\n",
      "-------------------------------------------\n",
      "('world', 1)\n",
      "('ben', 1)\n",
      "('hello', 1)\n",
      "('ik', 1)\n",
      "('jens', 1)\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:15\n",
      "-------------------------------------------\n",
      "('bla', 3)\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:30\n",
      "-------------------------------------------\n",
      "('test', 2)\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:10:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:11:50\n",
      "-------------------------------------------\n",
      "\n",
      "^Ctage 0:>                                                          (0 + 1) / 1]\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=3>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o14.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/workspace_groep456/Week_10/networkwordcount.py\", line 20, in <module>\n",
      "    ssc.awaitTermination() # zorg ervoor dat de applicatie actief blijft\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/streaming/context.py\", line 239, in awaitTermination\n",
      "    self._jssc.awaitTermination()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o25.awaitTermination\n",
      "25/05/20 12:11:52 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n"
     ]
    }
   ],
   "source": [
    "!python3 networkwordcount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StreamingContext in plaats van SparkContext\n",
    "\n",
    "Net zoals de context moet er een streaming context aangemaakt worden voor je de streaming api kan gebruiken.\n",
    "Een aantal belangrijke punten om te onthouden zijn:\n",
    "* Eens de context gestart is kan er geen nieuwe code toegevoegd worden\n",
    "* Eens de context gestopt is kan de context niet opnieuw gestart worden. \n",
    "* Er kan maar 1 context tegelijkertijd actief zijn\n",
    "* De spark context kan hergebruikt worden zolang de streaming context gestopt is voor er een nieuwe streamingcontext aangemaakt wordt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DStreams of Discretized Streams\n",
    "\n",
    "De basis abstractielaag gebruikt en voorzien door Spark Streaming.\n",
    "Dit stelt een continue datastroom voor die afkomstig is van de inputbron of de verwerkte datastroom van de transform stap.\n",
    "Deze stream stelt een continue tijdreeks voor van RDD's.\n",
    "Elk van deze RDD's stelt de ontvangen data tijdens een interval voor.\n",
    "\n",
    "Deze steams kunnen van verscheidene bronnen komen. \n",
    "Indien je een niet-standaard geincludeerde bron wil gebruiken moet je een eigen **receiver** schrijven om de data van de bron op te halen.\n",
    "Meer informatie over deze procedure vind je [hier](https://spark.apache.org/docs/latest/streaming-custom-receivers.html)\n",
    "\n",
    "Belangrijk om te onthouden dat het aantal beschikbare cores groter moet zijn dan het aantal receivers/gebruikte databronnen.\n",
    "Anders beschikt spark/de cluster niet over voldoende rekencapaciteiten om alles parallel uit te voeren.\n",
    "\n",
    "**Beschikbare transformaties**\n",
    "\n",
    "De meeste zaken die op een DataFrame/RDD uitgevoerd kunnen worden, kunnen ook op DStreams uitgevoerd worden. \n",
    "Een aantal operaties die extra aandacht vereisen zijn\n",
    "* updateStateByKey()\n",
    "* transform()\n",
    "* Window operations\n",
    "* Join operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UpdateStateByKey()**\n",
    "\n",
    "Deze functie maakt het mogelijk om een algemene state bij te houden en up te daten bij het ontvangen van nieuwe informatie.\n",
    "Voor het bovenstaande wordcount example kan dit bijvoorbeeld de wordcount van de volledige stream zijn ipv per lijn.\n",
    "Pas nu het wordcount-example aan door deze twee zaken bij te houden.\n",
    "Meer informatie kan je [hier](https://spark.apache.org/docs/latest/streaming-programming-guide.html#updatestatebykey-operation) vinden.\n",
    "\n",
    "**Tip:** Het is nodig om checkpointing te configureren voor deze functie kan gebruikt worden (zodat het ergens kan bijgehouden worden). Dit gebeurt door de volgende lijn na het aanmaken van de streaming context te plaatsen:\n",
    "    \n",
    "    ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting networkwordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file networkwordcount.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from datetime import datetime\n",
    "# tip: werk met scripts die in bestanden gaan geplaatst worden indien je werkt met streaming\n",
    "# maakt het makkelijker om streams te stoppen\n",
    "# deze bestanden moeten standalone zijn: maak de sparkcontext opnieuw aan + zorg voor de nodige imports\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 5 second\n",
    "sc = SparkContext(\"local[2]\", \"networkwordcount\")\n",
    "sc.setLogLevel(\"ERROR\") # reduce spam of logging\n",
    "ssc = StreamingContext(sc, 5) # om de 5 seconden\n",
    "\n",
    "# ALS JE WERKT MET STATE MOET JE DIT TOEVOEGEN\n",
    "ssc.checkpoint('checkpoint') # argument is een pad waar de checkpoints bewaard worden\n",
    "\n",
    "lines = ssc.socketTextStream('localhost', 19999) # de poort waarop deze applicatie luistert naar binnenkomende berichten\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "pairs = words.map(lambda x: (x,datetime.now())) # in het geval we werken met de timestampss\n",
    "counts = pairs.reduceByKey(lambda x,y : x+y)\n",
    "\n",
    "# hou een state bij\n",
    "def updateFunction(new_values, running_count):\n",
    "    # concept lijkt om het maken van een accumulator\n",
    "    return sum(new_values) + (running_count or 0) # nieuwe waarde + oude waarde (of 0 als het de eerste is)\n",
    "\n",
    "# overschrijf de vorige functie\n",
    "def updateFunction(new_values, timestamp):\n",
    "    # return de laatste keer dat een woord is gezien geweest\n",
    "    return datetime.now()\n",
    "\n",
    "# overschrijf de vorige functie\n",
    "def updateFunction(new_values, timestamps):\n",
    "    # return de laatste keer dat een woord is gezien geweest\n",
    "    if timestamps is None:\n",
    "        timestamps = []\n",
    "    timestamps.extend(new_values)\n",
    "    return timestamps\n",
    "    \n",
    "runningCounts = counts.updateStateByKey(updateFunction)\n",
    "\n",
    "runningCounts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination() # zorg ervoor dat de applicatie actief blijft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/20 12:24:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/usr/local/lib/python3.10/dist-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:24:45\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:24:50\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:24:55\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:00\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:05\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:10\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:15\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:20\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:25\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:30\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:35\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:40\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:45\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:50\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:25:55\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:26:00\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:26:05\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:26:10\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:26:15\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:26:20\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:26:25\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2025-05-20 12:26:30\n",
      "-------------------------------------------\n",
      "('test', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785881), datetime.datetime(2025, 5, 20, 12, 25, 5, 192709)])\n",
      "('world', [datetime.datetime(2025, 5, 20, 12, 24, 45, 785898)])\n",
      "\n",
      "^Ctage 0:>                                                          (0 + 1) / 1]\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=3>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o14.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/workspace_groep456/Week_10/networkwordcount.py\", line 44, in <module>\n",
      "    ssc.awaitTermination() # zorg ervoor dat de applicatie actief blijft\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/streaming/context.py\", line 239, in awaitTermination\n",
      "    self._jssc.awaitTermination()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o25.awaitTermination\n",
      "25/05/20 12:26:34 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n"
     ]
    }
   ],
   "source": [
    "!python3 networkwordcount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform operation**\n",
    "\n",
    "De transform operations laat je toe om een RDD-to-RDD functie toe te passenop een DStream.\n",
    "Dit laat je toe om alle RDD operaties toe te passen die niet zouden aangeboden worden door de Stream API\n",
    "Het is hierbij belangrijk om op te merken dat deze functie elke batch opgeroepen wordt en dus dat het mogelijk is om parameters te wijzigen tussen de verschillende batches (aantal partities, broadcasted variabelen, ...)\n",
    "\n",
    "    cleanedDStream = wordCounts.transform(lambda rdd: rdd.join(spamInfoRDD).filter(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Window operations**\n",
    "\n",
    "Een belangrijke eigenschap van streams is ook dat alle informatie binnen een bepaald tijdsvenster belangrijk kan zijn.\n",
    "Binnen spark streams zijn er een verscheidene WindowOperations om data binnen een bepaald window te aggregeren en te verwerken.\n",
    "Hieronder staat een voorbeeld om een reduce toe te passen om de 10 seconden op data dat in de laatste 30 seconden is binnengekomen.\n",
    "\n",
    "    windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join operations**\n",
    "\n",
    "Twee streams kunnen gecombineerd worden door middel van de .join() functie.\n",
    "Dit doet standaard een inner join maar andere mogelijkheden kunnen ook gekozen worden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending data to external systems\n",
    "\n",
    "De foreachRDD functie is een krachtige functie dat toegepast wordt op elke RDD dat aangemaakt wordt door een DStream. \n",
    "Dit maakt het mogelijk om de data uit te sturen naar externe systemen en zorgt dus voor de Load-stap binnen ETL.\n",
    "Een simplistische oplossing is als volgt:\n",
    "\n",
    "    def sendRecord(rdd):\n",
    "        connection = createNewConnection()  # executed at the driver\n",
    "        rdd.foreach(lambda record: connection.send(record))\n",
    "        connection.close()\n",
    "\n",
    "    dstream.foreachRDD(sendRecord)\n",
    "    \n",
    "Bovenstaande gaat niet werken omdat de connectie door de driver aangemaakt wordt.\n",
    "Deze connectie gaat geserializeerd worden en doorgestuurd naar de nodes maar dit gaat zelden correct lukken.\n",
    "Connectieproblemen bij het verzenden van data komen bijna steeds voort uit het correct aanmaken op de juiste nodes van de connectie.\n",
    "Een oplossing hiervoor is het volgende:\n",
    "\n",
    "    def sendRecord(record):\n",
    "        connection = createNewConnection()\n",
    "        connection.send(record)\n",
    "        connection.close()\n",
    "\n",
    "    dstream.foreachRDD(lambda rdd: rdd.foreach(sendRecord))\n",
    "    \n",
    "Dit gaat correct werken maar is echter suboptimaal omdat in deze code, een nieuwe connectie aangemaakt wordt voor elke rij in de stream wat voor heel veel overhead zorgt.\n",
    "Een andere mogelijkheid is als volgt\n",
    "\n",
    "    def sendPartition(iter):\n",
    "        connection = createNewConnection()\n",
    "        for record in iter:\n",
    "            connection.send(record)\n",
    "        connection.close()\n",
    "\n",
    "    dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))\n",
    "    \n",
    "Dit is reeds beter omdat de connectie reeds gedeeltelijk hergebruikt wordt maar wordt nog steeds herhaadelijk geopend en gesloten.\n",
    "Dit zorgt nog steeds voor onnodige overhead.\n",
    "De beste oplossing is door gebruik te maken van een connectionPool() waaruit connectie kunnen hergebruikt worden.\n",
    "Deze pool maakt automatisch connecties uit en sluit de bestaande connecties enkel indien ze voldoende lang ongebruikt worden.\n",
    "\n",
    "    def sendPartition(iter):\n",
    "        # ConnectionPool is a static, lazily initialized pool of connections\n",
    "        connection = ConnectionPool.getConnection()\n",
    "        for record in iter:\n",
    "            connection.send(record)\n",
    "        # return to the pool for future reuse\n",
    "        ConnectionPool.returnConnection(connection)\n",
    "\n",
    "    dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))\n",
    "\n",
    "dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints\n",
    "\n",
    "Checkpoints is een manier om informatie op te slaan op de cluster om spark fout-tolerant te worden voor crashes van zowel de driver als individuele node.\n",
    "Dit gebeurt door de nodige informatie op te slaan in een directory.\n",
    "\n",
    "Checkpointing moet ge-enabled worden wanneer je\n",
    "* een state wilt bijhouden\n",
    "* een fout-tolerante applicatie wil\n",
    "\n",
    "Let wel op dat het geen garantie is dat alle data behouden blijft maar het merendeel zou correct moeten opgevangen worden.\n",
    "\n",
    "Checkpointing toevoegen aan je applicatie gebeurt door een directory mee te gevan aan de sparkContext waar de checkpoints in een fout-tolerant gedistribueerd opslagsysteem kunnen bijgehouden worden.\n",
    "Dit gebeurd als volgt\n",
    "\n",
    "    ssc.checkpoint(\"checkpoints\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared variables with checkpoints**\n",
    "\n",
    "Accumulators en broadcasted variabelen worden niet opgeslagen door het checkpointing systeem in Spark. \n",
    "Dit kan opgelost worden door singleton instances te maken zodat ze kunnen geherinstantieerd worden nadat de driver restart na een failure.\n",
    "Onderstaande code is een voorbeeld van hoe dit uit te voeren in een wordcount example.\n",
    "Dit voorbeeld maakt gebruik van globals() wat de globale variabelen bijhoudt.\n",
    "In dit voorbeeld wordt er gebruik gemaakt van een broadcasted array om een lijst mee te geven met beginletters van woorden die genegeerd worden.\n",
    "Daarnaast wordt een accumulator gebruikt om het totaal aantal genegeerde woorden te tellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file networkwordcount.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# broadcast variabele met te negeren woorden\n",
    "def getWordExcludeList(sparkContext):\n",
    "    if (\"wordExcludeList\" not in globals()):\n",
    "        globals()[\"wordExcludeList\"] = sparkContext.broadcast([\"h\", \"v\"])\n",
    "    return globals()[\"wordExcludeList\"]\n",
    "\n",
    "# accumulator om het aantal genegeerde woorden dat tegengekomen wordt telt\n",
    "def getDroppedWordsCounter(sparkContext):\n",
    "    if (\"droppedWordsCounter\" not in globals()):\n",
    "        globals()[\"droppedWordsCounter\"] = sparkContext.accumulator(0)\n",
    "    return globals()[\"droppedWordsCounter\"]\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 5 second\n",
    "sc = SparkContext(\"local[2]\", \"networkwordcount\")\n",
    "sc.setLogLevel(\"ERROR\") # reduce spam of logging\n",
    "ssc = StreamingContext(sc, 5)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "lines = ssc.socketTextStream(\"localhost\", 19999)\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# de uit te voeren functie\n",
    "def echo(time, rdd):\n",
    "    # Get or register the excludeList Broadcast\n",
    "    excludeList = getWordExcludeList(rdd.context)\n",
    "    # Get or register the droppedWordsCounter Accumulator\n",
    "    droppedWordsCounter = getDroppedWordsCounter(rdd.context)\n",
    "    \n",
    "    print(excludeList.value)\n",
    "\n",
    "    # Use excludeList to drop words and use droppedWordsCounter to count them\n",
    "    def filterFunc(wordCount):\n",
    "        w = wordCount[0]\n",
    "        if len(w)>0 and w[0] in excludeList.value:\n",
    "            droppedWordsCounter.add(wordCount[1])\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    f = rdd.filter(filterFunc)\n",
    "\n",
    "    print(\"# Genegeerde woorden: %d\" % droppedWordsCounter.value)\n",
    "    print(\"Filtered rdd:\", str(f.collect()))\n",
    "\n",
    "wordCounts.foreachRDD(echo)\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Schrijf een streaming applicatie dat de volgende kenmerken heeft\n",
    "* Bereken het aantal woorden dat toekomt met een bepaalde lengte: bvb: \"Hello world over there\" => 1 woord met 4 karakters, 3 woorden met 5 karakters\n",
    "* Zorg voor checkpoints in een temp_state directory\n",
    "* Zorg ervoor dat dit berekend worden in sliding windows van 5 seconden dat elke 2 seconden opschuift\n",
    "* Hou een algemene state bij met het totaal aantal gelezen woorden, gebruik je hiervoor een accumulator of een update state by key?\n",
    "\n",
    "### Oplossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file oefening.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zijn de resultaten wat je verwacht had?\n",
    "Indien niet, hou zou je het kunnen oplossen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# antwoord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured streaming\n",
    "\n",
    "Naast het originele streaming systeem van spark gebruikmakende van DStreams, is er ook een [Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) variant.\n",
    "Het grootste verschil is dat terwijl DStreams gebaseerd zijn op RDD's objecten maakt structured streaming gebruik van DataFrames.\n",
    "Het networkcount dat we eerst aangehaald hadden hierboven ziet er als volgt uit met structured streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting structuredNetworkCount.py\n"
     ]
    }
   ],
   "source": [
    "%%file structuredNetworkCount.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('les').getOrCreate()\n",
    "\n",
    "lines = spark.readStream.format('socket').option('host', 'localhost').option('port', 19999).load()\n",
    "\n",
    "# splits in woorden\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'))\n",
    "# genereer aantal\n",
    "wordCounts = words.groupby('word').count()\n",
    "\n",
    "# output\n",
    "query = wordCounts.writeStream.outputMode('update').format('console').start()\n",
    "# forEachBatch\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/20 13:16:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/20 13:16:10 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "25/05/20 13:16:11 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-43e51973-7344-4e37-a3ae-c60cee3ce52c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/20 13:16:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "-------------------------------------------                                     \n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|hello|    1|\n",
      "|world|    1|\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|hello|    2|\n",
      "|class|    1|\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|world|    3|\n",
      "+-----+-----+\n",
      "\n",
      "^C\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=3>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o14.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/workspace_groep456/Week_10/structuredNetworkCount.py\", line 16, in <module>\n",
      "    query.awaitTermination()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\", line 221, in awaitTermination\n",
      "    return self._jsq.awaitTermination()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o47.awaitTermination\n"
     ]
    }
   ],
   "source": [
    "!python3 structuredNetworkCount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk op dat we hier de outputMode complete hebben gebruikt.\n",
    "Het resulterende gedrag is gelijkaardig aan de state van bij DStreams.\n",
    "Als we echter ook de wordcount per batch willen weten kunnen we gebruik maken van de andere modes.\n",
    "Onderstaande voorbeeld toont hoe het networkCount voorbeeld na te bootsen (namelijk via de update mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file structuredNetworkCount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Gebruik de informatie uit [deze link](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) en hermaak de vorige oefening van DStreams.\n",
    "Aangezien structured streaming een globaal dataframe bijhoudt moet er een timestamp toegevoegd worden om het te kunnen verdelen in windows.\n",
    "Dit moet gedaan worden in de source dus kunnen we hier met deze data geen windows bestuderen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file oefeningStructured.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
